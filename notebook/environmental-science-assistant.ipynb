{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"00b72124fb66433db64ee2e2ca371ca2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afcf91d872514eaba668455c9ef71227","placeholder":"​","style":"IPY_MODEL_33177e20b83a4a4494f31687b4e49a88","value":"Generating train split: "}},"042831c881f14ff186d0c4f60ac770e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"055477ee2f77428bbd47863df56edb13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61d9368e19d84dcab7d248de047e3350","IPY_MODEL_1e0170dae494477ca1beca3d6fc101da","IPY_MODEL_18d7e466c5754b24afe3ae3ac2c73ba4"],"layout":"IPY_MODEL_227268e2da824a90bc1a37a78be4cae6"}},"062347086d6e4fd0bf481891eedb8e66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8b8a25a502440279aaafc31cf602167","max":551,"min":0,"orientation":"horizontal","style":"IPY_MODEL_147634deca3441719173e4a5d5b29064","value":551}},"0704c0f58616425492a44a73755e7e16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a54875df9d748ea889a979cc312d475":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a7eda2baa3d4fe283d37b0eee00358a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ca3eaf6c7b34c028f0b47f606f74c7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13873f146dbd4dd1b26f16926a81500a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"147634deca3441719173e4a5d5b29064":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"16b16039c0a74ff6b390e2f703115c38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d36f484f6cd4290a7704bfa09cf1aa9","IPY_MODEL_5f3b9ce6239e4584ba418773f29e1291","IPY_MODEL_9c102e2a1b994c73b33341c85ee5ea93"],"layout":"IPY_MODEL_8773005b14ce4ce68d6dd2da9672156a"}},"1821934b81964e0290eca0f965f8d05f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18b11e87031645018e82fde659bfd96a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18d7e466c5754b24afe3ae3ac2c73ba4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6853792d8dbb403fbe2ce17f1a673bb6","placeholder":"​","style":"IPY_MODEL_777daebf4fcd408eaf508d627d0e6816","value":" 2736/0 [00:00&lt;00:00, 37091.70 examples/s]"}},"19cc961f7b2f40e0b48b1647995016fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d307e2e5ec74bf983942c1b6c0aad43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0170dae494477ca1beca3d6fc101da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be1bac0888624b858a5efb70ea3ddbf6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1821934b81964e0290eca0f965f8d05f","value":1}},"1f9cbbbc1cd347ee9dd19ae625b1cf00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7f5f417ff6d4f05ad4ed99abf21edf3","placeholder":"​","style":"IPY_MODEL_0a7eda2baa3d4fe283d37b0eee00358a","value":"tokenizer.model: 100%"}},"2013ba62aa7b404c98a8874dc2468a7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21bc59e369ec44bba6d07fc162c074b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2213210940864652a5bb561c72f82f39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_686f109f52134640b59df59d30a4f381","placeholder":"​","style":"IPY_MODEL_6a9c4ec3e35b474593545c3b0f3b96e7","value":" 124/124 [00:00&lt;00:00, 2.53kB/s]"}},"227268e2da824a90bc1a37a78be4cae6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227ecd44aa8347819c2599e0e168f9fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7779604e241f429abe044a4ac6761c5f","IPY_MODEL_062347086d6e4fd0bf481891eedb8e66","IPY_MODEL_434b92d1eb7a40209531f66dda8a30fe"],"layout":"IPY_MODEL_c0d6de8b0aa34e44b26dc7ab48aa7adf"}},"24c248eb62724ab0ad1eb179aab14d05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24c5f1ab3b464eb3b6d27e7e390d8fc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e140068c663483fb1dfb98f892867b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ffac522534c4bd7802172039931a69a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30465d1047aa4e29bf0496f32805fe16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13873f146dbd4dd1b26f16926a81500a","placeholder":"​","style":"IPY_MODEL_75969103b592487984f18139025a7d27","value":" 2736/2736 [00:01&lt;00:00, 1558.05 examples/s]"}},"304cf9ebcfd049df8b7c923b1a5e7cec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3063b6f228f448db9b4b1b9a3e6e8ec6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33177e20b83a4a4494f31687b4e49a88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38ff1f6f0c894961b0f23d7604a05085":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8460638fc24a4cea985747472e23661f","max":305,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f00f29693b0458abea827a08c916685","value":305}},"3e0d714ea3014db285a4c6bcd374bcc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ef082cf4f8e441e9ad6488a8978cb47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f5644397f944c39b902d04a1178259f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"416acbe0d87b4193bd69dcbe21374541":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"434547265e144717b63f2780fa94ff69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6ac51c7c9534a7795796ce805cec8ee","IPY_MODEL_6ee474fab5fb4800a75841d23973dbeb","IPY_MODEL_30465d1047aa4e29bf0496f32805fe16"],"layout":"IPY_MODEL_be793a9613e6401a965c09878c1915e4"}},"434b92d1eb7a40209531f66dda8a30fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9325647f098e45538876110485e3790d","placeholder":"​","style":"IPY_MODEL_f2bd34870018469d9f06ca84372f4241","value":" 551/551 [00:00&lt;00:00, 24.3kB/s]"}},"4707393f57a04cbf81bb608cd01950fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ae94e62569e485880c13f3e2d9d00e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c26d5155f964060ae302773ef3ecd2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5a585dfabaf473b8864149f7fac0a9b","placeholder":"​","style":"IPY_MODEL_6d69988f6a6e4893be09cc2b07135439","value":"generation_config.json: 100%"}},"4da0819f63e04db997e09616babe7957":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fdde65f3b014ff9b5d7f4977cea7228":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"526163674ec947e395f0c99e99675bd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"556888e9fca343c3ae129abd0a4b86ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56210125fb46446780cd9a6af1e0587e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56f2ae21aa96449089963e4a77d04c82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ccec3ffcfda47cb9a08050d38d15e88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d871d6e6c449489dae6953c4301f4eb4","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0704c0f58616425492a44a73755e7e16","value":499723}},"5dbb031f1659491192b3b420352f82ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aab782056c7045a5badf6ecd3cfefb82","IPY_MODEL_7f218cbeb6de4e69b9556a170179c2fd","IPY_MODEL_a8b5872c928841a2b31170f84ccb46b3"],"layout":"IPY_MODEL_21bc59e369ec44bba6d07fc162c074b0"}},"5dc51b199eea45ff8de35150ab4af249":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71b4c8c10c9642228e675e8c81a208bf","max":201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be1833aeb99f4b0995831d9751c6fec1","value":201}},"5dcc466c37554071b39599448a687fe1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3304b565c724af18a81e4538b72a88c","IPY_MODEL_ae9552ad26034e3395f7625d301ece64","IPY_MODEL_820141481e5d48fc89b7bffb1bbed73c"],"layout":"IPY_MODEL_8c4303aa12da47f2a0c5e573e95779bd"}},"5de68c8e927a48a0b5221ea51bea98a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00b72124fb66433db64ee2e2ca371ca2","IPY_MODEL_824b7ce5692f4dd596da18ef55b9abc9","IPY_MODEL_f89087e6a2994505a99c54463a7ad0df"],"layout":"IPY_MODEL_730cd7b25f874f73b58788f55861b421"}},"5f3b9ce6239e4584ba418773f29e1291":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96188557ade64a5cade07713915bce1d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_042831c881f14ff186d0c4f60ac770e3","value":1}},"61d9368e19d84dcab7d248de047e3350":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_964bba0db7f944fab6768323614e4cb8","placeholder":"​","style":"IPY_MODEL_8848a107f1c94803ac210053e747a342","value":"Generating train split: "}},"63351176013c41af9c2cb656439a10c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18b11e87031645018e82fde659bfd96a","placeholder":"​","style":"IPY_MODEL_2013ba62aa7b404c98a8874dc2468a7e","value":" 608/608 [00:00&lt;00:00, 14.2kB/s]"}},"6418b69291564023b05fe538fdf95072":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64677c404b104835bda781cb5771f6bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"676d6baffba9450e93eca96114bad96a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6853792d8dbb403fbe2ce17f1a673bb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"686f109f52134640b59df59d30a4f381":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a9c4ec3e35b474593545c3b0f3b96e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ab9916282da4229a42cf2012a992950":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d36f484f6cd4290a7704bfa09cf1aa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5250e947b7045c8b843430390519333","placeholder":"​","style":"IPY_MODEL_82373a10f2f2474eb0d2d1d6008b2536","value":"tokenizer.json: "}},"6d69988f6a6e4893be09cc2b07135439":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e42623483504182a018e47ffac847e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ae94e62569e485880c13f3e2d9d00e4","max":2200119864,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6be2dabb4414b93b5810edf167da1db","value":2200119864}},"6ee474fab5fb4800a75841d23973dbeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e140068c663483fb1dfb98f892867b1","max":2736,"min":0,"orientation":"horizontal","style":"IPY_MODEL_304cf9ebcfd049df8b7c923b1a5e7cec","value":2736}},"6f75d4654afa4c7aad053c4edfd6eb9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71867c676d344acd968a2cd6d813c28b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71b4c8c10c9642228e675e8c81a208bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"730cd7b25f874f73b58788f55861b421":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73bae91edce64bd2abaf7f92bf1b8f2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c57481f6605549449b85a95c2b67ea2d","placeholder":"​","style":"IPY_MODEL_3e0d714ea3014db285a4c6bcd374bcc8","value":" 305/305 [00:00&lt;00:00, 1534.74 examples/s]"}},"749305b03bfd40768ca522711cbc4e38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75969103b592487984f18139025a7d27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"761545708ae2451a985e30e099dc57db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ca7b11334bc4bb1bfb5d2c771e836be","IPY_MODEL_38ff1f6f0c894961b0f23d7604a05085","IPY_MODEL_73bae91edce64bd2abaf7f92bf1b8f2b"],"layout":"IPY_MODEL_e55bf98a0bf34a27b31dac22f13eb473"}},"7674db5aa3f54d509be69581bf901bd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64677c404b104835bda781cb5771f6bb","placeholder":"​","style":"IPY_MODEL_f929fd8d14f94766b2a5e785c328c357","value":" 500k/500k [00:01&lt;00:00, 348kB/s]"}},"7779604e241f429abe044a4ac6761c5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_749305b03bfd40768ca522711cbc4e38","placeholder":"​","style":"IPY_MODEL_24c5f1ab3b464eb3b6d27e7e390d8fc3","value":"special_tokens_map.json: 100%"}},"777daebf4fcd408eaf508d627d0e6816":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79b688732d5744199eb62b4e89275024":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf5a0c8fb1c4497bb5891a187beb9c21","placeholder":"​","style":"IPY_MODEL_0ca3eaf6c7b34c028f0b47f606f74c7f","value":"Loading weights: 100%"}},"7af36bc19fcc407b89614c63db8620ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89f100b5db60490fa54a13b6fa4b6384","IPY_MODEL_6e42623483504182a018e47ffac847e1","IPY_MODEL_a5e8f19f9d6b420fb57666326ca90bc7"],"layout":"IPY_MODEL_6418b69291564023b05fe538fdf95072"}},"7db1d568e6644a299e4615bb02796850":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f00f29693b0458abea827a08c916685":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f218cbeb6de4e69b9556a170179c2fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a76cc1496f6843b483db665ac5ffbea1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19cc961f7b2f40e0b48b1647995016fc","value":1}},"820141481e5d48fc89b7bffb1bbed73c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb52b2e9d52844dd8156e55347583bea","placeholder":"​","style":"IPY_MODEL_3063b6f228f448db9b4b1b9a3e6e8ec6","value":" 201/201 [00:06&lt;00:00, 32.87it/s, Materializing param=model.norm.weight]"}},"82373a10f2f2474eb0d2d1d6008b2536":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"824b7ce5692f4dd596da18ef55b9abc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ec915440beb43d183861790ca243bd1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71867c676d344acd968a2cd6d813c28b","value":1}},"8460638fc24a4cea985747472e23661f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8773005b14ce4ce68d6dd2da9672156a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8848a107f1c94803ac210053e747a342":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8857c0c3d303449791bdf25323e67bc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2619a8cc6414a4fa79d7f78281688d4","IPY_MODEL_a697e345fece4d10a082e61a7288b1dc","IPY_MODEL_63351176013c41af9c2cb656439a10c6"],"layout":"IPY_MODEL_3ef082cf4f8e441e9ad6488a8978cb47"}},"890fd6960f304a2aae0dc1d959b8bc5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89f100b5db60490fa54a13b6fa4b6384":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d70b5240ccc842e9a1cbbf8b68c347a9","placeholder":"​","style":"IPY_MODEL_6f75d4654afa4c7aad053c4edfd6eb9c","value":"model.safetensors: 100%"}},"8c4303aa12da47f2a0c5e573e95779bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ec915440beb43d183861790ca243bd1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9325647f098e45538876110485e3790d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9378f05b484d4aef80fa937be3bfd019":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec294483c3c5423cb0abe4d88b73e879","placeholder":"​","style":"IPY_MODEL_fddde1a91d15490886a7a623b93cd1e7","value":"Loading weights: 100%"}},"96188557ade64a5cade07713915bce1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"964bba0db7f944fab6768323614e4cb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bbae733a10b406891c7794d5e0cc4a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f9cbbbc1cd347ee9dd19ae625b1cf00","IPY_MODEL_5ccec3ffcfda47cb9a08050d38d15e88","IPY_MODEL_7674db5aa3f54d509be69581bf901bd4"],"layout":"IPY_MODEL_890fd6960f304a2aae0dc1d959b8bc5d"}},"9c102e2a1b994c73b33341c85ee5ea93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ab9916282da4229a42cf2012a992950","placeholder":"​","style":"IPY_MODEL_fb6337c171e74ca7b4e17ffd81d18aca","value":" 1.84M/? [00:00&lt;00:00, 11.6MB/s]"}},"9ca7b11334bc4bb1bfb5d2c771e836be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f880db1f1627456584045e11750f62ec","placeholder":"​","style":"IPY_MODEL_f1ba28b691e34a8bb32cdd15595dcbfe","value":"Map: 100%"}},"a0ec61bc5f794551bdabf225619110de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3304b565c724af18a81e4538b72a88c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4707393f57a04cbf81bb608cd01950fa","placeholder":"​","style":"IPY_MODEL_f090ba64a9c44147b5a6c87661c4ac3b","value":"Loading weights: 100%"}},"a5e8f19f9d6b420fb57666326ca90bc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7db1d568e6644a299e4615bb02796850","placeholder":"​","style":"IPY_MODEL_3f5644397f944c39b902d04a1178259f","value":" 2.20G/2.20G [00:21&lt;00:00, 182MB/s]"}},"a697e345fece4d10a082e61a7288b1dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24c248eb62724ab0ad1eb179aab14d05","max":608,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a54875df9d748ea889a979cc312d475","value":608}},"a76cc1496f6843b483db665ac5ffbea1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a8b5872c928841a2b31170f84ccb46b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc8331f210e74999b4a284c82e282a18","placeholder":"​","style":"IPY_MODEL_556888e9fca343c3ae129abd0a4b86ab","value":" 1.29k/? [00:00&lt;00:00, 26.5kB/s]"}},"aab782056c7045a5badf6ecd3cfefb82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da1290bc8cb946569874005ffa05edde","placeholder":"​","style":"IPY_MODEL_56f2ae21aa96449089963e4a77d04c82","value":"tokenizer_config.json: "}},"abad6df14f33408384e7826884d3e29f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae9552ad26034e3395f7625d301ece64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_416acbe0d87b4193bd69dcbe21374541","max":201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff8a6d6ab8094f1da8bc6ca360ec1e6c","value":201}},"afcf91d872514eaba668455c9ef71227":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b036abfa032f4998b3b051818a1937ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9378f05b484d4aef80fa937be3bfd019","IPY_MODEL_bbf654e99cf648f69d0defdc7ace050a","IPY_MODEL_c5bf6f6c6be3404391623dc79c242429"],"layout":"IPY_MODEL_526163674ec947e395f0c99e99675bd7"}},"b6ac51c7c9534a7795796ce805cec8ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ec61bc5f794551bdabf225619110de","placeholder":"​","style":"IPY_MODEL_4fdde65f3b014ff9b5d7f4977cea7228","value":"Map: 100%"}},"ba6caedbaa964a4abba74f1de832efb5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb52b2e9d52844dd8156e55347583bea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbf654e99cf648f69d0defdc7ace050a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c30e87fc9b14407a9d3ec904558d3e5d","max":201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ffac522534c4bd7802172039931a69a","value":201}},"be1833aeb99f4b0995831d9751c6fec1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be1bac0888624b858a5efb70ea3ddbf6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"be793a9613e6401a965c09878c1915e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0d6de8b0aa34e44b26dc7ab48aa7adf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1f63270156240ecbecefbcc7d918142":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c26d5155f964060ae302773ef3ecd2e","IPY_MODEL_df5834b542714609b173e28d257f51fc","IPY_MODEL_2213210940864652a5bb561c72f82f39"],"layout":"IPY_MODEL_1d307e2e5ec74bf983942c1b6c0aad43"}},"c30e87fc9b14407a9d3ec904558d3e5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c57481f6605549449b85a95c2b67ea2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5a585dfabaf473b8864149f7fac0a9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5bf6f6c6be3404391623dc79c242429":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca4bea7005ec455eb9707d37f11089b5","placeholder":"​","style":"IPY_MODEL_c622ac92e7ad4a71801da411c8241141","value":" 201/201 [00:04&lt;00:00, 42.07it/s, Materializing param=model.norm.weight]"}},"c622ac92e7ad4a71801da411c8241141":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c75479434a4a4a2cb4a6d066ed5b5da1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79b688732d5744199eb62b4e89275024","IPY_MODEL_5dc51b199eea45ff8de35150ab4af249","IPY_MODEL_fe036b3ee73a4cb1bc5e103f9bcaa161"],"layout":"IPY_MODEL_56210125fb46446780cd9a6af1e0587e"}},"ca4bea7005ec455eb9707d37f11089b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf5a0c8fb1c4497bb5891a187beb9c21":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d70b5240ccc842e9a1cbbf8b68c347a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d871d6e6c449489dae6953c4301f4eb4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da1290bc8cb946569874005ffa05edde":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df5834b542714609b173e28d257f51fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abad6df14f33408384e7826884d3e29f","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea2d1127bd7f48198355e965ac7b05cb","value":124}},"e2619a8cc6414a4fa79d7f78281688d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4ea8fe136644e089f1813ff53d9f45e","placeholder":"​","style":"IPY_MODEL_676d6baffba9450e93eca96114bad96a","value":"config.json: 100%"}},"e5250e947b7045c8b843430390519333":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e55bf98a0bf34a27b31dac22f13eb473":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7f5f417ff6d4f05ad4ed99abf21edf3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea2d1127bd7f48198355e965ac7b05cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ebeebdcaec1140848059c0206abac2b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec294483c3c5423cb0abe4d88b73e879":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f090ba64a9c44147b5a6c87661c4ac3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1ba28b691e34a8bb32cdd15595dcbfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2bd34870018469d9f06ca84372f4241":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4ea8fe136644e089f1813ff53d9f45e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6be2dabb4414b93b5810edf167da1db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f880db1f1627456584045e11750f62ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f89087e6a2994505a99c54463a7ad0df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4da0819f63e04db997e09616babe7957","placeholder":"​","style":"IPY_MODEL_ebeebdcaec1140848059c0206abac2b0","value":" 305/0 [00:00&lt;00:00, 10793.83 examples/s]"}},"f8b8a25a502440279aaafc31cf602167":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f929fd8d14f94766b2a5e785c328c357":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb6337c171e74ca7b4e17ffd81d18aca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc8331f210e74999b4a284c82e282a18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fddde1a91d15490886a7a623b93cd1e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe036b3ee73a4cb1bc5e103f9bcaa161":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba6caedbaa964a4abba74f1de832efb5","placeholder":"​","style":"IPY_MODEL_fe845cac9beb484489ed9923d0955856","value":" 201/201 [00:05&lt;00:00, 34.14it/s, Materializing param=model.norm.weight]"}},"fe845cac9beb484489ed9923d0955856":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff8a6d6ab8094f1da8bc6ca360ec1e6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14963098,"datasetId":9577423,"databundleVersionId":15834518},{"sourceType":"datasetVersion","sourceId":14965943,"datasetId":9579273,"databundleVersionId":15837619}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environmental Science & Circular Economy Assistant\n\n## Project Overview\n\nThis project fine-tunes a Large Language Model (LLM) to create a domain-specific assistant focused on **environmental science, sustainability, and circular economy practices**. The assistant provides accurate, scientific answers to questions about environmental topics and upcycling, while appropriately handling out-of-domain queries.\n\n## Purpose\n\nThe chatbot serves as an educational and practical tool that:\n- **Answers questions** about environmental science, climate change, sustainability, and ecological systems\n- **Provides upcycling guidance** with practical tips for repurposing common waste materials\n- **Politely refuses** questions outside its domain expertise, maintaining focus on environmental topics\n- **Educates users** about circular economy principles and sustainable practices\n\n## Domain Justification\n\n**Why Environmental Science & Circular Economy?**\n\n1. **Critical Global Challenge**: Climate change and environmental degradation are among the most pressing issues of our time, requiring accessible educational tools\n2. **Practical Impact**: Upcycling and circular economy practices offer immediate, actionable solutions that individuals can implement\n3. **Knowledge Gap**: Many people lack accessible, accurate information about environmental science and sustainable practices\n4. **Educational Value**: This assistant bridges the gap between scientific knowledge and practical application, making complex environmental concepts accessible\n\n## Model Choice: TinyLlama\n\n**Why TinyLlama/TinyLlama-1.1B-Chat-v1.0?**\n\n- **Colab-Compatible**: At 1.1B parameters, it fits comfortably within Google Colab's free GPU memory limits (T4 GPU ~15GB)\n- **Chat-Optimized**: Pre-trained on chat format, making it ideal for conversational Q&A tasks\n- **Efficient Fine-tuning**: Small size allows for faster training and experimentation with hyperparameters\n- **Modern Architecture**: Based on LLaMA architecture, providing good performance despite smaller size\n- **Parameter-Efficient**: Works well with LoRA (Low-Rank Adaptation) for efficient fine-tuning on limited hardware\n\n---\n\n## Project Structure\n\nThis notebook follows a complete LLM fine-tuning pipeline:\n1. **Data Collection & Preprocessing** - Loading and cleaning domain-specific datasets\n2. **Data Augmentation** - Adding upcycling examples and refusal handling\n3. **Model Fine-tuning** - Using LoRA for parameter-efficient training\n4. **Evaluation** - Quantitative metrics and qualitative testing\n5. **Deployment** - Gradio UI for user interaction","metadata":{"id":"baJiWdhrrAaQ"}},{"cell_type":"markdown","source":"## 1. Data Collection & Loading\n\n### Dataset Sources\n\nThis project combines multiple data sources to create a comprehensive training dataset:\n\n#### Primary Dataset: GeoGPT Environmental Q&A\n- **Source**: GeoGPT dataset containing environmental science question-answer pairs from academic papers\n- **Format**: CSV file with columns: `question`, `answer`, `title`, `authors`, `journal`, etc.\n- **Size**: ~41,432 total samples (we sample 3,000 for training efficiency)\n- **Why chosen**:\n  - High-quality, scientifically accurate answers from peer-reviewed sources\n  - Covers diverse environmental topics (climate, water quality, biodiversity, etc.)\n  - Provides domain-specific knowledge base for the assistant\n\n#### Augmentation Dataset: Manual Upcycling Q&A\n- **Source**: Manually curated examples (30 samples)\n- **Purpose**: Adds practical upcycling knowledge not typically found in academic papers\n- **Why needed**:\n  - Bridges gap between scientific knowledge and practical application\n  - Provides actionable advice for circular economy practices\n  - Enhances model's ability to answer \"how-to\" questions about waste reduction\n\n#### Refusal Training Data: Out-of-Domain Examples\n- **Source**: Manually created examples (120 samples)\n- **Purpose**: Teaches the model to politely refuse non-environmental questions\n- **Why critical**:\n  - Maintains domain focus and prevents hallucination\n  - Improves user experience by providing clear boundaries\n  - Demonstrates responsible AI behavior","metadata":{"id":"zo7AWSCOXBWP"}},{"cell_type":"code","source":"# Install required libraries\n!pip -q install datasets pandas scikit-learn\n\n# Import libraries\nfrom datasets import load_dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom google.colab import drive\n","metadata":{"id":"ud_PySgR9b0y","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:26:50.705945Z","iopub.execute_input":"2026-02-26T02:26:50.706256Z","iopub.status.idle":"2026-02-26T02:27:00.095130Z","shell.execute_reply.started":"2026-02-26T02:26:50.706233Z","shell.execute_reply":"2026-02-26T02:27:00.094311Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\ngeo_df = pd.read_csv(\"/kaggle/input/datasets/favourakinwande/geo-dataset/geogpt-qa.csv\")\n\n# Basic dataset information\nprint(\"Dataset loaded successfully\")\nprint(\"\\nDataset shape:\", geo_df.shape)\n\n# Show column names\nprint(\"\\nColumns in dataset:\")\nprint(list(geo_df.columns))\n\n# Check missing values\nprint(\"\\nMissing values per column:\")\nprint(geo_df.isnull().sum())\n\n# Data Validation: Check required columns exist\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DATA VALIDATION\")\nprint(\"=\" * 60)\nrequired_columns = ['question', 'answer']\nmissing_columns = [col for col in required_columns if col not in geo_df.columns]\n\nif missing_columns:\n    raise ValueError(f\"Missing required columns: {missing_columns}. Available columns: {list(geo_df.columns)}\")\nelse:\n    print(\" Required columns ('question', 'answer') found\")\n\n# Validate data quality\nif len(geo_df) == 0:\n    raise ValueError(\"Dataset is empty!\")\nprint(f\" Dataset contains {len(geo_df)} rows\")\n\nif geo_df['question'].isna().all() or geo_df['answer'].isna().all():\n    raise ValueError(\"Critical columns ('question' or 'answer') are completely empty!\")\nprint(\" Required columns contain data\")\n\nprint(\"=\" * 60)\n\n# Preview first rows\ngeo_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:00.096738Z","iopub.execute_input":"2026-02-26T02:27:00.097225Z","iopub.status.idle":"2026-02-26T02:27:01.214135Z","shell.execute_reply.started":"2026-02-26T02:27:00.097196Z","shell.execute_reply":"2026-02-26T02:27:01.213364Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully\n\nDataset shape: (41432, 10)\n\nColumns in dataset:\n['index', 'question', 'answer', 'title', 'authors', 'doi', 'journal', 'volume', 'pages', 'license']\n\nMissing values per column:\nindex           0\nquestion        0\nanswer          0\ntitle           0\nauthors        28\ndoi         32746\njournal         0\nvolume          0\npages        8686\nlicense         0\ndtype: int64\n\n============================================================\nDATA VALIDATION\n============================================================\n Required columns ('question', 'answer') found\n Dataset contains 41432 rows\n Required columns contain data\n============================================================\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   index                                           question  \\\n0      0  What are the key factors affecting air quality...   \n1      1  How does the spatial dependence of air quality...   \n2      2  What is the impact of tourism development on t...   \n3      3  How does tourism development in Indonesia impa...   \n4      4  What does 76.9% fluidity loss indicate in the ...   \n\n                                              answer  \\\n0  The key factors affecting air quality in Kalim...   \n1  The spatial dependence of air quality in Kalim...   \n2  Tourism development in the Karanganyar-Magetan...   \n3  Tourism development in Indonesia, particularly...   \n4  76.9% fluidity loss indicates a significant de...   \n\n                                               title  \\\n0  Spatial effects on air quality due to the capi...   \n1  Spatial effects on air quality due to the capi...   \n2  Regional collaboration in forest sustainabilit...   \n3  Regional collaboration in forest sustainabilit...   \n4  Fluidity deterioration of Batu Ayau coking coa...   \n\n                                             authors  \\\n0               D.M. Nihayah;F.M. Sundoro;L Masluhah   \n1               D.M. Nihayah;F.M. Sundoro;L Masluhah   \n2                                        Isti Andini   \n3                                        Isti Andini   \n4  Agus Haris Widayat;Komang Anggayana;Edy Sanwan...   \n\n                                       doi  \\\n0  doi.org/10.1088/1755-1315/1438/1/012052   \n1  doi.org/10.1088/1755-1315/1438/1/012052   \n2  doi.org/10.1088/1755-1315/1462/1/012068   \n3  doi.org/10.1088/1755-1315/1462/1/012068   \n4  doi.org/10.1088/1755-1315/1486/1/012031   \n\n                                             journal       volume  pages  \\\n0  IOP Conference Series: Earth and Environmental...  Volume 1438    NaN   \n1  IOP Conference Series: Earth and Environmental...  Volume 1438    NaN   \n2  IOP Conference Series: Earth and Environmental...  Volume 1462    NaN   \n3  IOP Conference Series: Earth and Environmental...  Volume 1462    NaN   \n4  IOP Conference Series: Earth and Environmental...  Volume 1486    NaN   \n\n  license  \n0   CC-BY  \n1   CC-BY  \n2   CC-BY  \n3   CC-BY  \n4   CC-BY  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>title</th>\n      <th>authors</th>\n      <th>doi</th>\n      <th>journal</th>\n      <th>volume</th>\n      <th>pages</th>\n      <th>license</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>What are the key factors affecting air quality...</td>\n      <td>The key factors affecting air quality in Kalim...</td>\n      <td>Spatial effects on air quality due to the capi...</td>\n      <td>D.M. Nihayah;F.M. Sundoro;L Masluhah</td>\n      <td>doi.org/10.1088/1755-1315/1438/1/012052</td>\n      <td>IOP Conference Series: Earth and Environmental...</td>\n      <td>Volume 1438</td>\n      <td>NaN</td>\n      <td>CC-BY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>How does the spatial dependence of air quality...</td>\n      <td>The spatial dependence of air quality in Kalim...</td>\n      <td>Spatial effects on air quality due to the capi...</td>\n      <td>D.M. Nihayah;F.M. Sundoro;L Masluhah</td>\n      <td>doi.org/10.1088/1755-1315/1438/1/012052</td>\n      <td>IOP Conference Series: Earth and Environmental...</td>\n      <td>Volume 1438</td>\n      <td>NaN</td>\n      <td>CC-BY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>What is the impact of tourism development on t...</td>\n      <td>Tourism development in the Karanganyar-Magetan...</td>\n      <td>Regional collaboration in forest sustainabilit...</td>\n      <td>Isti Andini</td>\n      <td>doi.org/10.1088/1755-1315/1462/1/012068</td>\n      <td>IOP Conference Series: Earth and Environmental...</td>\n      <td>Volume 1462</td>\n      <td>NaN</td>\n      <td>CC-BY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>How does tourism development in Indonesia impa...</td>\n      <td>Tourism development in Indonesia, particularly...</td>\n      <td>Regional collaboration in forest sustainabilit...</td>\n      <td>Isti Andini</td>\n      <td>doi.org/10.1088/1755-1315/1462/1/012068</td>\n      <td>IOP Conference Series: Earth and Environmental...</td>\n      <td>Volume 1462</td>\n      <td>NaN</td>\n      <td>CC-BY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>What does 76.9% fluidity loss indicate in the ...</td>\n      <td>76.9% fluidity loss indicates a significant de...</td>\n      <td>Fluidity deterioration of Batu Ayau coking coa...</td>\n      <td>Agus Haris Widayat;Komang Anggayana;Edy Sanwan...</td>\n      <td>doi.org/10.1088/1755-1315/1486/1/012031</td>\n      <td>IOP Conference Series: Earth and Environmental...</td>\n      <td>Volume 1486</td>\n      <td>NaN</td>\n      <td>CC-BY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# # Mount Google Drive\n# drive.mount('/content/drive')\n\n# # Path to your GeoGPT CSV in Google Drive\n# file_path = \"/content/drive/MyDrive/geogpt-qa.csv\"\n\n# # Load dataset into pandas DataFrame\n# geo_df = pd.read_csv(file_path)\n\n# # Basic dataset information\n# print(\"Dataset loaded successfully\")\n# print(\"\\nDataset shape:\", geo_df.shape)\n\n# # Show column names\n# print(\"\\nColumns in dataset:\")\n# print(list(geo_df.columns))\n\n# # Check missing values\n# print(\"\\nMissing values per column:\")\n# print(geo_df.isnull().sum())\n\n# # Data Validation: Check required columns exist\n# print(\"\\n\" + \"=\" * 60)\n# print(\"DATA VALIDATION\")\n# print(\"=\" * 60)\n# required_columns = ['question', 'answer']\n# missing_columns = [col for col in required_columns if col not in geo_df.columns]\n\n# if missing_columns:\n#     raise ValueError(f\"Missing required columns: {missing_columns}. Available columns: {list(geo_df.columns)}\")\n# else:\n#     print(\" Required columns ('question', 'answer') found\")\n\n# # Validate data quality\n# if len(geo_df) == 0:\n#     raise ValueError(\"Dataset is empty!\")\n# print(f\" Dataset contains {len(geo_df)} rows\")\n\n# if geo_df['question'].isna().all() or geo_df['answer'].isna().all():\n#     raise ValueError(\"Critical columns ('question' or 'answer') are completely empty!\")\n# print(\" Required columns contain data\")\n\n# print(\"=\" * 60)\n\n# # Preview first rows\n# geo_df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":935},"id":"bphpbFpiKyFH","outputId":"5b5f80cd-1b09-4175-f543-4ca128323cc6","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:01.215217Z","iopub.execute_input":"2026-02-26T02:27:01.215465Z","iopub.status.idle":"2026-02-26T02:27:01.219323Z","shell.execute_reply.started":"2026-02-26T02:27:01.215444Z","shell.execute_reply":"2026-02-26T02:27:01.218747Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Dataset Overview\n\nThe GeoGPT dataset contains **41,432 question-answer pairs** from environmental science research papers. Key characteristics:\n\n- **Coverage**: Topics include climate change, water quality, biodiversity, air pollution, soil science, and more\n- **Quality**: Answers are extracted from peer-reviewed academic sources, ensuring scientific accuracy\n- **Format**: Structured as question-answer pairs, perfect for fine-tuning a conversational assistant\n\n**Note**: We sample 3,000 rows for training to balance:\n- **Model performance** (sufficient data for learning)\n- **Colab constraints** (memory and training time limits)\n- **Training efficiency** (faster iteration for hyperparameter tuning)","metadata":{"id":"cd9W_0wlrAaW"}},{"cell_type":"markdown","source":"## 2. Data Preprocessing & Cleaning\n\n### Preprocessing Steps\n\nThe following steps ensure high-quality training data:\n\n1. **Column Extraction**: Extract only `question` and `answer` columns needed for training\n2. **Type Conversion**: Convert to string type and strip whitespace\n3. **Empty Removal**: Remove rows with empty questions or answers\n4. **Deduplication**: Remove duplicate question-answer pairs\n5. **Sampling**: Sample 3,000 rows for Colab-friendly training size\n\n### Why These Steps Matter\n\n- **String conversion & stripping**: Ensures consistent data types and removes formatting artifacts\n- **Empty removal**: Prevents training on invalid examples that would confuse the model\n- **Deduplication**: Avoids data leakage and ensures diverse training examples\n- **Sampling**: Balances dataset size with computational constraints while maintaining diversity","metadata":{"id":"m--tKsoRrAaX"}},{"cell_type":"code","source":"# Extract only the columns needed for training (question and answer)\nqa_df = geo_df[[\"question\", \"answer\"]].copy()\n\n# Convert to string and remove extra spaces\nqa_df[\"question\"] = qa_df[\"question\"].astype(str).str.strip()\nqa_df[\"answer\"] = qa_df[\"answer\"].astype(str).str.strip()\n\n# Remove rows with empty question or answer\nqa_df = qa_df[(qa_df[\"question\"] != \"\") & (qa_df[\"answer\"] != \"\")]\n\n# Remove duplicate question-answer pairs\nqa_df = qa_df.drop_duplicates(subset=[\"question\", \"answer\"]).reset_index(drop=True)\n\n# Show cleaned dataset size\nprint(\"Clean dataset size:\", len(qa_df))\n\n# Data Validation: Verify qa_df structure\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DATA VALIDATION - Cleaned Dataset\")\nprint(\"=\" * 60)\nif len(qa_df) == 0:\n    raise ValueError(\"Cleaned dataset is empty! Check data cleaning steps.\")\nprint(f\"Cleaned dataset contains {len(qa_df)} rows\")\n\nif 'question' not in qa_df.columns or 'answer' not in qa_df.columns:\n    raise ValueError(\"Missing required columns in qa_df\")\nprint(\"Required columns present\")\n\n# Check if we have enough data for sampling\nif len(qa_df) < 3000:\n    print(f\"Dataset has only {len(qa_df)} rows, less than requested 3000. Using all available data.\")\n    qa_sample = qa_df.copy().reset_index(drop=True)\nelse:\n    # Sample 3000 rows for training (recommended for Colab)\n    qa_sample = qa_df.sample(n=3000, random_state=42).reset_index(drop=True)\n\nprint(f\"Sampled dataset size: {len(qa_sample)}\")\nprint(\"=\" * 60)\n\n# Preview sample\nqa_sample.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362},"id":"65xcT2WxMmBm","outputId":"7078dede-7f45-41a1-da39-c989fdd89bcc","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:01.221017Z","iopub.execute_input":"2026-02-26T02:27:01.221668Z","iopub.status.idle":"2026-02-26T02:27:01.400432Z","shell.execute_reply.started":"2026-02-26T02:27:01.221638Z","shell.execute_reply":"2026-02-26T02:27:01.399656Z"}},"outputs":[{"name":"stdout","text":"Clean dataset size: 40819\n\n============================================================\nDATA VALIDATION - Cleaned Dataset\n============================================================\nCleaned dataset contains 40819 rows\nRequired columns present\nSampled dataset size: 3000\n============================================================\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  How did the environmental conditions in 2020, ...   \n1  How can geotagged social media data be used in...   \n2  What were the findings of the water quality in...   \n3  How does adjusting the shortwave scaling facto...   \n4  How are unmanned aerial vehicles (UAVs) being ...   \n\n                                              answer  \n0  The environmental conditions in 2020, particul...  \n1  Geotagged social media data, a form of volunte...  \n2  Between March 2016 and June 2017, an investiga...  \n3  By adding a scaling factor of 0.75 to the shor...  \n4  Unmanned aerial vehicles (UAVs) have been incr...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How did the environmental conditions in 2020, ...</td>\n      <td>The environmental conditions in 2020, particul...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How can geotagged social media data be used in...</td>\n      <td>Geotagged social media data, a form of volunte...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What were the findings of the water quality in...</td>\n      <td>Between March 2016 and June 2017, an investiga...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How does adjusting the shortwave scaling facto...</td>\n      <td>By adding a scaling factor of 0.75 to the shor...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How are unmanned aerial vehicles (UAVs) being ...</td>\n      <td>Unmanned aerial vehicles (UAVs) have been incr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Data Augmentation\n\n### Adding Domain-Specific Knowledge\n\nTo enhance the model's capabilities, we add two types of specialized data:\n\n#### A. Upcycling Q&A Examples\n- **30 manually curated examples** covering common upcycling scenarios\n- **Purpose**: Provides practical, actionable advice on circular economy practices\n- **Topics**: Food waste, plastic bottles, glass jars, cardboard, textiles, etc.\n- **Why important**: Academic datasets rarely include practical \"how-to\" upcycling guidance\n\n#### B. Refusal Training Examples  \n- **120 examples** of out-of-domain questions with polite refusal responses\n- **Purpose**: Teaches the model to maintain domain boundaries\n- **Examples**: General knowledge (sports, politics), programming, unrelated topics\n- **Why critical**: Prevents the model from hallucinating answers outside its expertise","metadata":{"id":"A6oMt5hhrAaX"}},{"cell_type":"code","source":"import pandas as pd\n\n# Upcycling Q&A (30 samples)\nupcycling_examples = [\n    (\"What can banana peels be upcycled into?\", \"Banana peels can be upcycled into compost, compost tea, natural enzyme cleaner, or dried peel powder for soil enrichment.\"),\n    (\"How can plastic bottles be upcycled?\", \"Plastic bottles can be upcycled into planters, drip irrigation bottles, eco-bricks, storage containers, or craft materials.\"),\n    (\"How can glass jars be reused?\", \"Glass jars can be reused for storage, candle holders, vases, spice containers, or small terrariums.\"),\n    (\"How can cardboard be upcycled?\", \"Cardboard can be turned into storage dividers, seedling trays, packaging filler, or compost material.\"),\n    (\"How can old clothes be upcycled?\", \"Old clothes can be turned into tote bags, cleaning rags, pillow covers, or reusable shopping bags.\"),\n    (\"How can old jeans be upcycled?\", \"Old jeans can be transformed into tote bags, aprons, pencil cases, patchwork blankets, or cleaning cloths.\"),\n    (\"How can coffee grounds be reused?\", \"Coffee grounds can be added to compost, used as a deodorizer, or used as a mild soil amendment.\"),\n    (\"How can eggshells be reused?\", \"Eggshells can be crushed and added to compost, used as a soil amendment, or used as seed starters.\"),\n    (\"How can citrus peels be reused?\", \"Citrus peels can be used in compost, infused in vinegar for natural cleaner, or used as deodorizer.\"),\n    (\"How can food scraps be upcycled?\", \"Food scraps can be converted into compost, compost tea, or used to regrow certain vegetables and herbs.\"),\n    (\"How can wooden pallets be upcycled?\", \"Wooden pallets can be turned into furniture, shelves, garden beds, compost bins, or storage racks.\"),\n    (\"How can paper waste be reused?\", \"Paper waste can be reused for scrap notes, crafts, packaging filler, or recycled paper.\"),\n    (\"How can broken ceramics be reused?\", \"Broken ceramics can be used for mosaic art, garden decoration, or creative craft projects.\"),\n    (\"How can tires be reused safely?\", \"Old tires can be used as outdoor planters, swings, or garden barriers, but should not be burned.\"),\n    (\"How can metal cans be reused?\", \"Metal cans can be reused as pen holders, planters, candle containers, or small storage units.\"),\n    (\"How can plastic bags be reused?\", \"Plastic bags can be reused for storage, bin liners, or woven into reusable mats.\"),\n    (\"How can fabric scraps be reused?\", \"Fabric scraps can be turned into patchwork items, cleaning cloths, stuffing, or crafts.\"),\n    (\"How can glass bottles be reused?\", \"Glass bottles can be reused as vases, decorative items, storage containers, or DIY lamps.\"),\n    (\"How can old newspapers be reused?\", \"Old newspapers can be used for packing, compost material, seed starters, or crafts.\"),\n    (\"How can kitchen waste be reused?\", \"Kitchen waste can be converted into compost or natural cleaners.\"),\n    (\"How can coconut shells be reused?\", \"Coconut shells can be turned into bowls, plant pots, or decorative crafts.\"),\n    (\"How can wood scraps be reused?\", \"Wood scraps can be used for crafts, compost carbon material, or small DIY projects.\"),\n    (\"How can packaging materials be reused?\", \"Packaging materials can be reused for storage, crafts, or protective wrapping.\"),\n    (\"How can organic waste be reused?\", \"Organic waste can be converted into compost or natural fertilizer.\"),\n    (\"How can biodegradable waste be reused?\", \"Biodegradable waste can be composted and used as soil nutrients.\"),\n    (\"How can plastic containers be reused?\", \"Plastic containers can be reused for storage, planting, or organizing items.\"),\n    (\"How can cardboard boxes be reused?\", \"Cardboard boxes can be reused for storage, moving, or compost.\"),\n    (\"How can glass waste be reused?\", \"Glass waste can be remolded, reused in crafts, or used for storage.\"),\n    (\"How can metal scrap be reused?\", \"Metal scrap can be recycled or reused in DIY projects.\"),\n    (\"How can furniture be upcycled?\", \"Old furniture can be repaired, repainted, or redesigned into new functional items.\")\n]\n\nup_df = pd.DataFrame(upcycling_examples, columns=[\"question\", \"answer\"])\n\n\n# Refusal Q&A (120 samples)\nout_of_domain_questions = [\n    \"Who won the FIFA World Cup 2022?\",\n    \"What is the capital of France?\",\n    \"Write Python code to sort numbers\",\n    \"Explain quantum physics simply\",\n    \"Who is the president of the USA?\",\n    \"Solve 5x + 3 = 18\",\n    \"What is the tallest building in the world?\",\n    \"Write a poem about love\",\n    \"Explain blockchain technology\",\n    \"Give me relationship advice\",\n    \"What is machine learning?\",\n    \"Explain photosynthesis in detail\"\n]\n\nREFUSAL_ANSWER = (\n    \"I am a domain-specific assistant focused on environmental science and sustainability. \"\n    \"I cannot answer questions outside this domain. Please ask about environmental science, climate, or sustainability.\"\n)\n\nrefusal_rows = [(q, REFUSAL_ANSWER) for _ in range(10) for q in out_of_domain_questions]\nrefusal_df = pd.DataFrame(refusal_rows, columns=[\"question\", \"answer\"])\n\n\n# Combine into upcycling_dataset\nupcycling_dataset = pd.concat([up_df, refusal_df], ignore_index=True).drop_duplicates().reset_index(drop=True)\n\nprint(\"Upcycling rows:\", len(up_df))\nprint(\"Refusal rows:\", len(refusal_df))\nprint(\"Total extra rows:\", len(upcycling_dataset))\n\nupcycling_dataset.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"r_73vMYQNliQ","outputId":"93b5a02b-da61-4940-f042-4e4d9774b5db","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:01.401564Z","iopub.execute_input":"2026-02-26T02:27:01.401837Z","iopub.status.idle":"2026-02-26T02:27:01.419947Z","shell.execute_reply.started":"2026-02-26T02:27:01.401811Z","shell.execute_reply":"2026-02-26T02:27:01.419098Z"}},"outputs":[{"name":"stdout","text":"Upcycling rows: 30\nRefusal rows: 120\nTotal extra rows: 42\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                  question  \\\n0  What can banana peels be upcycled into?   \n1     How can plastic bottles be upcycled?   \n2            How can glass jars be reused?   \n3           How can cardboard be upcycled?   \n4         How can old clothes be upcycled?   \n\n                                              answer  \n0  Banana peels can be upcycled into compost, com...  \n1  Plastic bottles can be upcycled into planters,...  \n2  Glass jars can be reused for storage, candle h...  \n3  Cardboard can be turned into storage dividers,...  \n4  Old clothes can be turned into tote bags, clea...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What can banana peels be upcycled into?</td>\n      <td>Banana peels can be upcycled into compost, com...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How can plastic bottles be upcycled?</td>\n      <td>Plastic bottles can be upcycled into planters,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How can glass jars be reused?</td>\n      <td>Glass jars can be reused for storage, candle h...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How can cardboard be upcycled?</td>\n      <td>Cardboard can be turned into storage dividers,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How can old clothes be upcycled?</td>\n      <td>Old clothes can be turned into tote bags, clea...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Merge GeoGPT sample with additional data (Upcycling + Refusal)\n# Combine datasets\nfinal_df = pd.concat([qa_sample, upcycling_dataset], ignore_index=True)\n\n# Remove duplicates again (safety)\nfinal_df = final_df.drop_duplicates(subset=[\"question\", \"answer\"]).reset_index(drop=True)\n\n# Display dataset size\nprint(\"GeoGPT sample size:\", len(qa_sample))\nprint(\"Extra data size:\", len(upcycling_dataset))\nprint(\"Final merged dataset size:\", len(final_df))\n\n# Preview merged dataset\nfinal_df.sample(5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"hKhxSUU1OOek","outputId":"e535b05a-af3b-4655-fb56-54cc5b73edd5","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:01.420728Z","iopub.execute_input":"2026-02-26T02:27:01.420952Z","iopub.status.idle":"2026-02-26T02:27:01.445544Z","shell.execute_reply.started":"2026-02-26T02:27:01.420930Z","shell.execute_reply":"2026-02-26T02:27:01.444958Z"}},"outputs":[{"name":"stdout","text":"GeoGPT sample size: 3000\nExtra data size: 42\nFinal merged dataset size: 3042\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                               question  \\\n2460  How does the multi-hazard model help in identi...   \n689   How does the diurnal temperature range (DTR) a...   \n944   What is the impact of acid mine drainage on aq...   \n1828  How does prescribed burning, a common land man...   \n92    How does XRD contribute to the characterizatio...   \n\n                                                 answer  \n2460  The multi-hazard model helps in identifying ar...  \n689   In the Inner Mongolia Plain, the diurnal tempe...  \n944   In Central Europe, acid mine drainage (AMD) ha...  \n1828  Prescribed burning is a tool frequently used f...  \n92    XRD, along with bulk ICP-MS chemical analysis,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2460</th>\n      <td>How does the multi-hazard model help in identi...</td>\n      <td>The multi-hazard model helps in identifying ar...</td>\n    </tr>\n    <tr>\n      <th>689</th>\n      <td>How does the diurnal temperature range (DTR) a...</td>\n      <td>In the Inner Mongolia Plain, the diurnal tempe...</td>\n    </tr>\n    <tr>\n      <th>944</th>\n      <td>What is the impact of acid mine drainage on aq...</td>\n      <td>In Central Europe, acid mine drainage (AMD) ha...</td>\n    </tr>\n    <tr>\n      <th>1828</th>\n      <td>How does prescribed burning, a common land man...</td>\n      <td>Prescribed burning is a tool frequently used f...</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>How does XRD contribute to the characterizatio...</td>\n      <td>XRD, along with bulk ICP-MS chemical analysis,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA)\n\nLet's analyze the merged dataset to understand its characteristics:","metadata":{"id":"LZaN-4GBrAaY"}},{"cell_type":"code","source":"# Exploratory Data Analysis\ntry:\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    HAS_PLOTTING = True\nexcept ImportError:\n    print(\"Matplotlib/Seaborn not available. Installing...\")\n    !pip -q install matplotlib seaborn\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    HAS_PLOTTING = True\n\n# Dataset statistics\nprint(\"=\" * 60)\nprint(\"DATASET STATISTICS\")\nprint(\"=\" * 60)\nprint(f\"Total samples: {len(final_df)}\")\nprint(f\"Unique questions: {final_df['question'].nunique()}\")\nprint(f\"Unique answers: {final_df['answer'].nunique()}\")\nprint(f\"\\nQuestion length statistics:\")\nprint(f\"  Mean: {final_df['question'].str.len().mean():.1f} characters\")\nprint(f\"  Median: {final_df['question'].str.len().median():.1f} characters\")\nprint(f\"  Min: {final_df['question'].str.len().min()} characters\")\nprint(f\"  Max: {final_df['question'].str.len().max()} characters\")\nprint(f\"\\nAnswer length statistics:\")\nprint(f\"  Mean: {final_df['answer'].str.len().mean():.1f} characters\")\nprint(f\"  Median: {final_df['answer'].str.len().median():.1f} characters\")\nprint(f\"  Min: {final_df['answer'].str.len().min()} characters\")\nprint(f\"  Max: {final_df['answer'].str.len().max()} characters\")\n\n# Visualize length distributions\nif HAS_PLOTTING:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Question length distribution\n    axes[0].hist(final_df['question'].str.len(), bins=50, edgecolor='black', alpha=0.7)\n    axes[0].set_xlabel('Question Length (characters)')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Distribution of Question Lengths')\n    axes[0].axvline(final_df['question'].str.len().mean(), color='r', linestyle='--', label=f'Mean: {final_df[\"question\"].str.len().mean():.1f}')\n    axes[0].legend()\n\n    # Answer length distribution\n    axes[1].hist(final_df['answer'].str.len(), bins=50, edgecolor='black', alpha=0.7, color='green')\n    axes[1].set_xlabel('Answer Length (characters)')\n    axes[1].set_ylabel('Frequency')\n    axes[1].set_title('Distribution of Answer Lengths')\n    axes[1].axvline(final_df['answer'].str.len().mean(), color='r', linestyle='--', label=f'Mean: {final_df[\"answer\"].str.len().mean():.1f}')\n    axes[1].legend()\n\n    plt.tight_layout()\n    plt.show()\n\n# Sample examples from each category\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SAMPLE EXAMPLES\")\nprint(\"=\" * 60)\n\n# Environmental science examples\nprint(\"\\n Environmental Science Examples:\")\nenv_samples = final_df[~final_df['question'].str.contains('upcycl|reus|recycl', case=False, na=False)].head(3)\nfor idx, row in env_samples.iterrows():\n    print(f\"\\nQ: {row['question'][:100]}...\")\n    print(f\"A: {row['answer'][:150]}...\")\n\n# Upcycling examples\nprint(\"\\n Upcycling Examples:\")\nupcycle_samples = final_df[final_df['question'].str.contains('upcycl|reus|recycl', case=False, na=False)].head(3)\nfor idx, row in upcycle_samples.iterrows():\n    print(f\"\\nQ: {row['question']}\")\n    print(f\"A: {row['answer'][:150]}...\")\n\n# Refusal examples (if any are identifiable)\nrefusal_samples = final_df[final_df['answer'].str.contains('cannot answer|outside|domain', case=False, na=False)].head(2)\nif len(refusal_samples) > 0:\n    print(\"\\n Refusal Examples:\")\n    for idx, row in refusal_samples.iterrows():\n        print(f\"\\nQ: {row['question']}\")\n        print(f\"A: {row['answer']}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"t-QG7HnerAaZ","outputId":"9455bcfc-4fad-4104-a140-fa17ef727e79","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:01.447663Z","iopub.execute_input":"2026-02-26T02:27:01.447914Z","iopub.status.idle":"2026-02-26T02:27:02.419775Z","shell.execute_reply.started":"2026-02-26T02:27:01.447891Z","shell.execute_reply":"2026-02-26T02:27:02.419098Z"}},"outputs":[{"name":"stdout","text":"============================================================\nDATASET STATISTICS\n============================================================\nTotal samples: 3042\nUnique questions: 3042\nUnique answers: 3031\n\nQuestion length statistics:\n  Mean: 134.0 characters\n  Median: 128.0 characters\n  Min: 17 characters\n  Max: 329 characters\n\nAnswer length statistics:\n  Mean: 619.4 characters\n  Median: 578.5 characters\n  Min: 54 characters\n  Max: 2194 characters\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1400x500 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n============================================================\nSAMPLE EXAMPLES\n============================================================\n\n Environmental Science Examples:\n\nQ: How did the environmental conditions in 2020, particularly humidity and temperature, influence the d...\nA: The environmental conditions in 2020, particularly humidity and temperature, had a significant impact on both the development and mitigation stages of...\n\nQ: How can geotagged social media data be used in disaster management?...\nA: Geotagged social media data, a form of volunteered geographic information (VGI), has been widely explored to support the prevention, preparation, and ...\n\nQ: What were the findings of the water quality investigation in Aida Camp and Alazzah Camp between Marc...\nA: Between March 2016 and June 2017, an investigation into the water quality in Aida Camp and Alazzah Camp, two Palestinian refugee camps in the West Ban...\n\n Upcycling Examples:\n\nQ: How might the use of a hydrocyclone in the desliming process contribute to the efficient recycling of feldspar, quartz, and kaolin from weathered granite, and what are the potential environmental benefits of this approach?\nA: The use of a hydrocyclone in the desliming process is crucial for efficiently recycling feldspar, quartz, and kaolin from weathered granite. The hydro...\n\nQ: How does wastewater treatment impact the presence of antibiotic-resistant bacteria and genes in reused water?\nA: Wastewater treatment plays a crucial role in managing antibiotic-resistant bacteria (ARB) and antibiotic resistance genes (ARGs) in reused water. Howe...\n\nQ: How does the regulatory framework for produced water management and reuse in the U.S. vary across different regions, and what are the implications for public health and environmental protection?\nA: In the U.S., the regulatory framework for produced water (PW) management and reuse is complex and varies across different regions. The U.S. Environmen...\n\n Refusal Examples:\n\nQ: How can a unified frame improve the understanding of compound humidity-heat extremes?\nA: A unified frame can improve the understanding of compound humidity-heat extremes by combining compound humidity-heat extremes with their regional extension. This approach uses the evolution of daily mean 2 m air temperature (Tmean) and relative humidity (RH) over a large domain as two dynamical systems, and the instantaneous coupling index from the dynamical systems method combined with clustering analysis can identify regional compound humidity-heat extremes with distinct spatial organized structures.\n\nQ: How does the WRF-CMAQ model help in understanding the factors contributing to complex PM2.5 and O3 pollution episodes in North China Plain and what are the key findings from the study in Suqian regarding the role of meteorological conditions and transport processes?\nA: The WRF-CMAQ model plays a crucial role in understanding the factors contributing to complex PM2.5 and O3 pollution episodes. In the North China Plain, the model was used to analyze a regional PM2.5-O3 complex pollution episode from 3 to 5 April 2019. The study found that static and stable weather conditions with high temperature and low wind speed, despite weakening photochemical reactions of O3 near the ground due to high PM2.5 concentrations, allowed a large amount of O3 generated through gas-phase chemical reactions at high altitudes to be transported downwards, increasing ground-level O3 concentrations. This, in turn, facilitated the conversion of SO2 and NO2 into secondary inorganic salts and volatile organic compounds into secondary organic aerosols, amplifying PM2.5 concentrations and exacerbating air pollution. The contributions of transport from outside sources to PM2.5 (above 60%) and O3 (above 46%) increased significantly during the episode.\n\nIn Suqian, the WRF-CMAQ model was used to analyze an O3 pollution event from 18 to 21 June 2020. The study found that meteorological conditions such as temperature and wind field played a significant role in the formation and accumulation of O3. During the heavy pollution period, vertical transport (VTRA) and horizontal transport (HTRA) significantly enhanced O3 concentration. The photochemical reactions of precursors, such as NOx and VOCs transported from long distances, and O3 directly transported to Suqian from other regions, contributed greatly to O3 pollution, while local sources contributed very little, between 12.22% and 18.33%. Based on the simulation of 25 emission reduction scenarios, it was found that excessive emission reduction of NOx is not conducive to reducing O3 concentration, and it is best to control the emission reduction ratio at about 10%. It is recommended to reduce VOCs as much as possible, particularly those generated by traffic sources, without affecting normal production and life.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 4. Text Normalization & Quality Filtering\n\n### Normalization Function\n\nThe `normalize_text()` function performs essential text cleaning:\n\n- **Removes non-breaking spaces** (`\\u00a0`): Handles encoding issues from different sources\n- **Collapses whitespace**: Converts multiple spaces/tabs/newlines to single spaces\n- **Strips leading/trailing whitespace**: Ensures clean text boundaries\n\n### Why Normalization is Critical\n\n- **Encoding consistency**: Different data sources may use different character encodings\n- **Token efficiency**: Reduces unnecessary tokens from extra whitespace\n- **Model clarity**: Clean text helps the model learn patterns more effectively\n\n### Length Filtering\n\nWe filter answers to be between **20-2000 characters**:\n\n- **Minimum (20 chars)**: Removes very short, uninformative answers that don't provide value\n- **Maximum (2000 chars)**: Ensures answers fit within the model's context window (512 tokens ≈ ~2000 chars)\n- **Why important**: Prevents training on truncated or incomplete examples","metadata":{"id":"vZa_38fhrAaZ"}},{"cell_type":"code","source":"import re\n\n# Simple text cleanup for questions and answers\ndef normalize_text(s):\n    s = str(s)\n    s = s.replace(\"\\u00a0\", \" \")          # remove non-breaking spaces\n    s = re.sub(r\"\\s+\", \" \", s).strip()    # collapse extra whitespace\n    return s\n\n# Fill missing values before cleaning\nfinal_df[\"question\"] = final_df[\"question\"].fillna(\"\")\nfinal_df[\"answer\"] = final_df[\"answer\"].fillna(\"\")\n\n# Normalize the text\nfinal_df[\"question\"] = final_df[\"question\"].apply(normalize_text)\nfinal_df[\"answer\"] = final_df[\"answer\"].apply(normalize_text)\n\n# Remove empty rows\nfinal_df = final_df[(final_df[\"question\"] != \"\") & (final_df[\"answer\"] != \"\")].reset_index(drop=True)\n\n# Remove duplicate question-answer pairs\nfinal_df = final_df.drop_duplicates(subset=[\"question\", \"answer\"]).reset_index(drop=True)\n\n# Optional quality filters (helps rubric and training stability)\nfinal_df = final_df[final_df[\"answer\"].str.len() >= 20].reset_index(drop=True)   # remove very short answers\nfinal_df = final_df[final_df[\"answer\"].str.len() <= 2000].reset_index(drop=True) # remove extremely long answers\n\nprint(\"Rows after preprocessing:\", len(final_df))\n\n# Data Validation: Verify final_df structure before proceeding\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DATA VALIDATION - Final Dataset\")\nprint(\"=\" * 60)\nif len(final_df) == 0:\n    raise ValueError(\"Final dataset is empty after preprocessing! Check filtering steps.\")\nprint(f\"Final dataset contains {len(final_df)} rows\")\n\nrequired_cols = ['question', 'answer']\nmissing_cols = [col for col in required_cols if col not in final_df.columns]\nif missing_cols:\n    raise ValueError(f\"Missing required columns: {missing_cols}\")\nprint(\" Required columns ('question', 'answer') present\")\n\n# Check for empty questions/answers\nempty_q = final_df['question'].str.strip().eq('').sum()\nempty_a = final_df['answer'].str.strip().eq('').sum()\nif empty_q > 0 or empty_a > 0:\n    print(f\" Warning: Found {empty_q} empty questions and {empty_a} empty answers\")\nelse:\n    print(\" No empty questions or answers\")\n\n# Validate data types\nif not final_df['question'].dtype == 'object' or not final_df['answer'].dtype == 'object':\n    print(\"Warning: Expected string (object) dtype for question/answer columns\")\nelse:\n    print(\" Data types are correct (string/object)\")\n\nprint(\"=\" * 60)\n\nfinal_df.head(3)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"0vxK5r8HTWXI","outputId":"084ed089-939a-49b9-a6a5-26c6e0f5fde4","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:02.420784Z","iopub.execute_input":"2026-02-26T02:27:02.421080Z","iopub.status.idle":"2026-02-26T02:27:02.598427Z","shell.execute_reply.started":"2026-02-26T02:27:02.421049Z","shell.execute_reply":"2026-02-26T02:27:02.597825Z"}},"outputs":[{"name":"stdout","text":"Rows after preprocessing: 3041\n\n============================================================\nDATA VALIDATION - Final Dataset\n============================================================\nFinal dataset contains 3041 rows\n Required columns ('question', 'answer') present\n No empty questions or answers\n Data types are correct (string/object)\n============================================================\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  How did the environmental conditions in 2020, ...   \n1  How can geotagged social media data be used in...   \n2  What were the findings of the water quality in...   \n\n                                              answer  \n0  The environmental conditions in 2020, particul...  \n1  Geotagged social media data, a form of volunte...  \n2  Between March 2016 and June 2017, an investiga...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How did the environmental conditions in 2020, ...</td>\n      <td>The environmental conditions in 2020, particul...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How can geotagged social media data be used in...</td>\n      <td>Geotagged social media data, a form of volunte...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What were the findings of the water quality in...</td>\n      <td>Between March 2016 and June 2017, an investiga...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 5. Instruction Formatting\n\n### Chat Template Format\n\nWe format each training example using a structured template:\n\n```\n[SYSTEM] You are an environmental science and sustainability assistant...\n[USER] <question>\n[ASSISTANT] <answer>\n```\n\n### Why This Format?\n\n- **Matches TinyLlama's training**: TinyLlama-Chat was trained on similar chat formats\n- **Clear role separation**: Helps model understand context and expected response style\n- **System prompt guidance**: Sets domain boundaries and expected behavior\n- **Standard format**: Compatible with Hugging Face's chat templates and inference pipelines","metadata":{"id":"UfNcEr-erAaa"}},{"cell_type":"code","source":"# Data Validation: Verify final_df structure for training\n# Data Validation: Verify final_df structure BEFORE creating train_df\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DATA VALIDATION - Before Instruction Formatting\")\nprint(\"=\" * 60)\nif 'final_df' not in globals():\n    raise NameError(\"ERROR: 'final_df' is not defined! Run the merge and preprocessing cells first.\")\n\nif len(final_df) == 0:\n    raise ValueError(\"Final dataset is empty! Cannot proceed with training.\")\nprint(f\"Final dataset contains {len(final_df)} rows\")\n\n# Check that final_df has the required columns that will be renamed to instruction/response\n# Note: final_df has 'question' and 'answer', NOT 'instruction'/'response'/'text' yet\nrequired_cols = ['question', 'answer']\nmissing_cols = [col for col in required_cols if col not in final_df.columns]\nif missing_cols:\n    raise ValueError(f\"Missing required columns in final_df: {missing_cols}. Available columns: {list(final_df.columns)}\")\nprint(\"Required columns ('question', 'answer') present - ready for instruction formatting\")\n\nprint(\"=\" * 60)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWVBQqobrAaa","outputId":"684ee937-84ec-48de-990d-03bc12a7e76c","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:02.599234Z","iopub.execute_input":"2026-02-26T02:27:02.599519Z","iopub.status.idle":"2026-02-26T02:27:02.606025Z","shell.execute_reply.started":"2026-02-26T02:27:02.599495Z","shell.execute_reply":"2026-02-26T02:27:02.605353Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nDATA VALIDATION - Before Instruction Formatting\n============================================================\nFinal dataset contains 3041 rows\nRequired columns ('question', 'answer') present - ready for instruction formatting\n============================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# System prompt to guide assistant behavior\nSYSTEM_PROMPT = (\n    \"You are an environmental science and sustainability assistant. \"\n    \"Answer clearly with accurate scientific explanations. \"\n    \"If a question is outside environmental science or sustainability, politely refuse and suggest a relevant topic.\"\n)\n\n# Ensure no missing values before formatting\nfinal_df[\"question\"] = final_df[\"question\"].fillna(\"\")\nfinal_df[\"answer\"] = final_df[\"answer\"].fillna(\"\")\n\n# Rename for clarity\ntrain_df = final_df.rename(columns={\n    \"question\": \"instruction\",\n    \"answer\": \"response\"\n}).copy()\n\n# Build formatted training text\ntrain_df[\"text\"] = train_df.apply(\n    lambda r: (\n        f\"[SYSTEM] {SYSTEM_PROMPT}\\n\"\n        f\"[USER] {r['instruction']}\\n\"\n        f\"[ASSISTANT] {r['response']}\"\n    ),\n    axis=1\n)\n\nprint(\"Instruction-formatted rows:\", len(train_df))\ntrain_df.head(2)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":129},"id":"kQWbFUXeTbbu","outputId":"8adea4b3-fee1-4a33-f35d-ed4634e7eac9","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:02.607008Z","iopub.execute_input":"2026-02-26T02:27:02.607367Z","iopub.status.idle":"2026-02-26T02:27:02.659167Z","shell.execute_reply.started":"2026-02-26T02:27:02.607342Z","shell.execute_reply":"2026-02-26T02:27:02.658569Z"}},"outputs":[{"name":"stdout","text":"Instruction-formatted rows: 3041\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                         instruction  \\\n0  How did the environmental conditions in 2020, ...   \n1  How can geotagged social media data be used in...   \n\n                                            response  \\\n0  The environmental conditions in 2020, particul...   \n1  Geotagged social media data, a form of volunte...   \n\n                                                text  \n0  [SYSTEM] You are an environmental science and ...  \n1  [SYSTEM] You are an environmental science and ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>response</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How did the environmental conditions in 2020, ...</td>\n      <td>The environmental conditions in 2020, particul...</td>\n      <td>[SYSTEM] You are an environmental science and ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How can geotagged social media data be used in...</td>\n      <td>Geotagged social media data, a form of volunte...</td>\n      <td>[SYSTEM] You are an environmental science and ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## 6. Tokenization\n\n### Tokenizer Selection: TinyLlama Tokenizer\n\n**Why TinyLlama Tokenizer?**\n\n- **Architecture Match**: The tokenizer is specifically designed for TinyLlama models, ensuring perfect compatibility\n- **Chat Format Support**: Handles chat-style formatting (`[SYSTEM]`, `[USER]`, `[ASSISTANT]`) correctly\n- **Efficient**: Fast tokenization suitable for large-scale training\n- **Vocabulary**: Optimized vocabulary size for the 1.1B parameter model\n\n### Context Window: max_len = 512\n\n**Why 512 tokens?**\n\n- **Model Limitation**: TinyLlama's architecture supports up to 2048 tokens, but 512 is a practical limit for:\n  - **Memory efficiency**: Shorter sequences use less GPU memory\n  - **Training speed**: Faster training iterations\n  - **Colab constraints**: Fits comfortably within free tier limits\n- **Data Fit**: Most of our Q&A pairs fit within 512 tokens when formatted\n\n### Tokenization Process\n\n- **Truncation**: Sequences longer than 512 tokens are truncated (removes end)\n- **Padding**: Sequences shorter than 512 tokens are padded to batch size\n- **Special Tokens**: Adds `<pad>`, `<eos>`, and other special tokens as needed\n\n**Note**: We verify tokenization works correctly by checking token lengths on a sample batch.","metadata":{"id":"awni1p0FrAaa"}},{"cell_type":"code","source":"# Install transformers (run once if needed)\n!pip -q install transformers\n\nfrom transformers import AutoTokenizer\nimport numpy as np\n\n# Load TinyLlama tokenizer (matches your model choice)\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\n# Add a pad token if the tokenizer doesn't have one (needed for batching)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Set the context window limit required by your project setup\nmax_len = 512\n\n# Tokenize a small batch as a demonstration\nsample_texts = train_df[\"text\"].head(50).tolist()\ntokens = tokenizer(sample_texts, truncation=True, padding=True, max_length=max_len)\n\n# Show evidence that tokenization works and respects max_len\nlengths = [len(x) for x in tokens[\"input_ids\"]]\nprint(\"Tokenizer:\", model_name)\nprint(\"Max length:\", max_len)\nprint(\"Min token length:\", int(np.min(lengths)))\nprint(\"Median token length:\", int(np.median(lengths)))\nprint(\"Max token length:\", int(np.max(lengths)))\n\n# Show how many samples hit the max length (meaning they were truncated)\ntruncated_count = sum(l == max_len for l in lengths)\nprint(\"Samples truncated (hit max_len):\", truncated_count, \"out of\", len(lengths))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":405,"referenced_widgets":["8857c0c3d303449791bdf25323e67bc9","e2619a8cc6414a4fa79d7f78281688d4","a697e345fece4d10a082e61a7288b1dc","63351176013c41af9c2cb656439a10c6","3ef082cf4f8e441e9ad6488a8978cb47","f4ea8fe136644e089f1813ff53d9f45e","676d6baffba9450e93eca96114bad96a","24c248eb62724ab0ad1eb179aab14d05","0a54875df9d748ea889a979cc312d475","18b11e87031645018e82fde659bfd96a","2013ba62aa7b404c98a8874dc2468a7e","5dbb031f1659491192b3b420352f82ca","aab782056c7045a5badf6ecd3cfefb82","7f218cbeb6de4e69b9556a170179c2fd","a8b5872c928841a2b31170f84ccb46b3","21bc59e369ec44bba6d07fc162c074b0","da1290bc8cb946569874005ffa05edde","56f2ae21aa96449089963e4a77d04c82","a76cc1496f6843b483db665ac5ffbea1","19cc961f7b2f40e0b48b1647995016fc","fc8331f210e74999b4a284c82e282a18","556888e9fca343c3ae129abd0a4b86ab","16b16039c0a74ff6b390e2f703115c38","6d36f484f6cd4290a7704bfa09cf1aa9","5f3b9ce6239e4584ba418773f29e1291","9c102e2a1b994c73b33341c85ee5ea93","8773005b14ce4ce68d6dd2da9672156a","e5250e947b7045c8b843430390519333","82373a10f2f2474eb0d2d1d6008b2536","96188557ade64a5cade07713915bce1d","042831c881f14ff186d0c4f60ac770e3","6ab9916282da4229a42cf2012a992950","fb6337c171e74ca7b4e17ffd81d18aca","9bbae733a10b406891c7794d5e0cc4a0","1f9cbbbc1cd347ee9dd19ae625b1cf00","5ccec3ffcfda47cb9a08050d38d15e88","7674db5aa3f54d509be69581bf901bd4","890fd6960f304a2aae0dc1d959b8bc5d","e7f5f417ff6d4f05ad4ed99abf21edf3","0a7eda2baa3d4fe283d37b0eee00358a","d871d6e6c449489dae6953c4301f4eb4","0704c0f58616425492a44a73755e7e16","64677c404b104835bda781cb5771f6bb","f929fd8d14f94766b2a5e785c328c357","227ecd44aa8347819c2599e0e168f9fc","7779604e241f429abe044a4ac6761c5f","062347086d6e4fd0bf481891eedb8e66","434b92d1eb7a40209531f66dda8a30fe","c0d6de8b0aa34e44b26dc7ab48aa7adf","749305b03bfd40768ca522711cbc4e38","24c5f1ab3b464eb3b6d27e7e390d8fc3","f8b8a25a502440279aaafc31cf602167","147634deca3441719173e4a5d5b29064","9325647f098e45538876110485e3790d","f2bd34870018469d9f06ca84372f4241"]},"id":"4m4iuyFebvwV","outputId":"8f244983-f4b2-4035-f957-7a71d54391d5","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:02.659967Z","iopub.execute_input":"2026-02-26T02:27:02.660244Z","iopub.status.idle":"2026-02-26T02:27:37.270720Z","shell.execute_reply.started":"2026-02-26T02:27:02.660221Z","shell.execute_reply":"2026-02-26T02:27:37.269863Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c186a1aa991844b7ad0d5fa37395eac5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"169ba6fc9029424dae9ef795996b7704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf84cd2f0e0e4cc18cf5b455f2717ecf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089d9adc342b4fcbadeb8ea63414d8ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b840e7d4d8448f191ae84b5eb936dae"}},"metadata":{}},{"name":"stdout","text":"Tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0\nMax length: 512\nMin token length: 454\nMedian token length: 454\nMax token length: 454\nSamples truncated (hit max_len): 0 out of 50\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 7. Train/Test Split & Data Export\n\n### Split Strategy\n\n- **Train/Test Split**: 90% training, 10% testing (standard practice for ML)\n- **Random State**: Fixed seed (42) for reproducibility\n- **Purpose**:\n  - **Training set**: Used to fine-tune the model\n  - **Test set**: Used for evaluation metrics (perplexity, BLEU, ROUGE) and qualitative testing\n\n### Export Format: JSONL\n\n- **Format**: JSON Lines (one JSON object per line)\n- **Why JSONL**:\n  - Easy to load with Hugging Face `datasets` library\n  - Efficient for large datasets\n  - Human-readable for inspection\n  - Standard format for LLM training pipelines\n\nThe files `train.jsonl` and `test.jsonl` will be used in the fine-tuning section.","metadata":{"id":"nbkKfJiarAab"}},{"cell_type":"code","source":"# Split into train and test sets (test is used for evaluation)\ntrain_split, test_split = train_test_split(train_df, test_size=0.1, random_state=42)\n\n# Save as JSONL files (clean pipeline step for rubric + easy reload for training)\ntrain_split.to_json(\"train.jsonl\", orient=\"records\", lines=True)\ntest_split.to_json(\"test.jsonl\", orient=\"records\", lines=True)\n\nprint(\"Saved: train.jsonl and test.jsonl\")\nprint(\"Train rows:\", len(train_split))\nprint(\"Test rows:\", len(test_split))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8i9AmJ3Tvz9","outputId":"e65ba35f-f518-4088-e33f-94d03e061f3f","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:37.271897Z","iopub.execute_input":"2026-02-26T02:27:37.272613Z","iopub.status.idle":"2026-02-26T02:27:37.329382Z","shell.execute_reply.started":"2026-02-26T02:27:37.272563Z","shell.execute_reply":"2026-02-26T02:27:37.328566Z"}},"outputs":[{"name":"stdout","text":"Saved: train.jsonl and test.jsonl\nTrain rows: 2736\nTest rows: 305\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 8. Model Fine-tuning (LoRA)\n\nThis section fine-tunes the pre-trained **TinyLlama** model from Hugging Face using **parameter-efficient fine-tuning (PEFT)** with **LoRA** via the `peft` library, ensuring compatibility with Colab's free GPU (e.g. T4).\n\n### Rubric alignment\n- **Pre-trained LLM**: TinyLlama/TinyLlama-1.1B-Chat-v1.0 from Hugging Face\n- **Parameter-efficient fine-tuning**: LoRA (Low-Rank Adaptation) via `peft`\n- **Hyperparameter tuning**: Learning rate (1e-4 to 5e-5), batch size (2–4 with gradient accumulation), optimizer selection, and training epochs (1–3)\n- **Documentation**: GPU memory usage and training time tracked for each run\n- **Experiment table**: Multiple configurations compared (learning rate, batch size, epochs, optimizer); validation metrics (e.g. eval loss) and improvement over baseline documented","metadata":{"id":"GDovItJmOKRB"}},{"cell_type":"code","source":"# Install PEFT, accelerate, and transformers (required for LoRA fine-tuning)\n!pip -q install peft accelerate transformers datasets bitsandbytes\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nimport time\nimport pandas as pd\n\n# Ensure we have tokenizer and max_len (from Section 6; redefine here so this section can run standalone after Section 7)\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nmax_len = 512\nif 'tokenizer' not in dir() or tokenizer is None:\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\nprint(\"Tokenizer and max_len ready:\", model_name, \"max_len =\", max_len)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynTwyqQlOKRB","outputId":"b5975420-fc14-4278-b95d-decc12dab462","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:37.330456Z","iopub.execute_input":"2026-02-26T02:27:37.330733Z","iopub.status.idle":"2026-02-26T02:27:45.716412Z","shell.execute_reply.started":"2026-02-26T02:27:37.330708Z","shell.execute_reply":"2026-02-26T02:27:45.715574Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hTokenizer and max_len ready: TinyLlama/TinyLlama-1.1B-Chat-v1.0 max_len = 512\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Load train and test data from JSONL (produced in Section 7)\ntrain_ds = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\ntest_ds = load_dataset(\"json\", data_files=\"test.jsonl\", split=\"train\")\n\ndef tokenize_fn(examples):\n    out = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=max_len,\n        padding=\"max_length\",\n        return_tensors=None,\n    )\n    # Causal LM: labels = input_ids; mask padding (set to -100 so loss ignores them)\n    pad_id = tokenizer.pad_token_id\n    out[\"labels\"] = [[id_ if id_ != pad_id else -100 for id_ in seq] for seq in out[\"input_ids\"]]\n    return out\n\ntrain_tokenized = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\ntest_tokenized = test_ds.map(tokenize_fn, batched=True, remove_columns=test_ds.column_names)\ntrain_tokenized.set_format(\"torch\")\ntest_tokenized.set_format(\"torch\")\n\n# For causal LM: labels = input_ids (we predict next token); mask padding in loss\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\nprint(\"Train size:\", len(train_tokenized), \"| Test size:\", len(test_tokenized))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162,"referenced_widgets":["055477ee2f77428bbd47863df56edb13","61d9368e19d84dcab7d248de047e3350","1e0170dae494477ca1beca3d6fc101da","18d7e466c5754b24afe3ae3ac2c73ba4","227268e2da824a90bc1a37a78be4cae6","964bba0db7f944fab6768323614e4cb8","8848a107f1c94803ac210053e747a342","be1bac0888624b858a5efb70ea3ddbf6","1821934b81964e0290eca0f965f8d05f","6853792d8dbb403fbe2ce17f1a673bb6","777daebf4fcd408eaf508d627d0e6816","5de68c8e927a48a0b5221ea51bea98a6","00b72124fb66433db64ee2e2ca371ca2","824b7ce5692f4dd596da18ef55b9abc9","f89087e6a2994505a99c54463a7ad0df","730cd7b25f874f73b58788f55861b421","afcf91d872514eaba668455c9ef71227","33177e20b83a4a4494f31687b4e49a88","8ec915440beb43d183861790ca243bd1","71867c676d344acd968a2cd6d813c28b","4da0819f63e04db997e09616babe7957","ebeebdcaec1140848059c0206abac2b0","434547265e144717b63f2780fa94ff69","b6ac51c7c9534a7795796ce805cec8ee","6ee474fab5fb4800a75841d23973dbeb","30465d1047aa4e29bf0496f32805fe16","be793a9613e6401a965c09878c1915e4","a0ec61bc5f794551bdabf225619110de","4fdde65f3b014ff9b5d7f4977cea7228","2e140068c663483fb1dfb98f892867b1","304cf9ebcfd049df8b7c923b1a5e7cec","13873f146dbd4dd1b26f16926a81500a","75969103b592487984f18139025a7d27","761545708ae2451a985e30e099dc57db","9ca7b11334bc4bb1bfb5d2c771e836be","38ff1f6f0c894961b0f23d7604a05085","73bae91edce64bd2abaf7f92bf1b8f2b","e55bf98a0bf34a27b31dac22f13eb473","f880db1f1627456584045e11750f62ec","f1ba28b691e34a8bb32cdd15595dcbfe","8460638fc24a4cea985747472e23661f","7f00f29693b0458abea827a08c916685","c57481f6605549449b85a95c2b67ea2d","3e0d714ea3014db285a4c6bcd374bcc8"]},"id":"oij6_BBwOKRC","outputId":"ce321743-ca5e-4898-cd78-c155a37dfb27","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:45.717800Z","iopub.execute_input":"2026-02-26T02:27:45.718471Z","iopub.status.idle":"2026-02-26T02:27:47.878296Z","shell.execute_reply.started":"2026-02-26T02:27:45.718436Z","shell.execute_reply":"2026-02-26T02:27:47.877543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb1d4ce6d8949699edec9b72de7dafc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad729a8aece244f79869f6b8cd44f25f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2736 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b0aa162cbc940579c8c360193d6d961"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/305 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d06db4d9db4de78f581f27f733ab22"}},"metadata":{}},{"name":"stdout","text":"Train size: 2736 | Test size: 305\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Experiment log for rubric: document each run (hyperparameters, eval loss, GPU memory, time)\n# Imports needed so this cell works even if Section 8 imports cell was not run\nimport torch\nimport sys\nimport time\nfrom transformers import TrainerCallback, AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model, TaskType\nexperiment_log = []\n\nclass ProgressCallback(TrainerCallback):\n    \"\"\"Print training progress so you see it in Colab (step-based + log-based).\"\"\"\n    def __init__(self, total_steps_per_epoch, total_steps):\n        self.total_steps_per_epoch = total_steps_per_epoch\n        self.total_steps = total_steps\n    def on_epoch_begin(self, args, state, control, **kwargs):\n        epoch_num = int(state.epoch) + 1\n        print(f\"\\n--- Epoch {epoch_num} ---\", flush=True)\n        sys.stdout.flush()\n    def on_step_end(self, args, state, control, **kwargs):\n        step = state.global_step\n        if step % 5 == 0 or step == 1:\n            pct = 100 * step / self.total_steps if self.total_steps else 0\n            print(f\"  Step {step}/{self.total_steps} ({pct:.1f}%)\", flush=True)\n            sys.stdout.flush()\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is None:\n            return\n        step = state.global_step\n        loss = logs.get(\"loss\", None)\n        lr = logs.get(\"learning_rate\", None)\n        eval_loss = logs.get(\"eval_loss\", None)\n        if eval_loss is not None:\n            print(f\"  [Eval] eval_loss={eval_loss:.4f}\", flush=True)\n        elif isinstance(loss, (int, float)):\n            s = f\"  [Step {step}] loss={loss:.4f}\"\n            if lr is not None:\n                s += f\"  lr={lr:.2e}\"\n            print(s, flush=True)\n        else:\n            print(f\"  [Step {step}] {logs}\", flush=True)\n        sys.stdout.flush()\n\ndef run_training(run_id, learning_rate=5e-5, per_device_train_batch_size=2, per_device_eval_batch_size=2,\n                 gradient_accumulation_steps=4, num_epochs=2, lora_r=8, lora_alpha=16, output_dir=\"./out\"):\n    \"\"\"Run one training experiment and append to experiment_log. Tracks GPU memory and time.\"\"\"\n    global model_name, tokenizer, train_tokenized, test_tokenized, data_collator\n    if \"model_name\" not in globals():\n        model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    if \"tokenizer\" not in globals():\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n    if \"train_tokenized\" not in globals():\n        from datasets import load_dataset\n        max_len = 512\n        train_ds = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\n        test_ds = load_dataset(\"json\", data_files=\"test.jsonl\", split=\"train\")\n        def _tokenize(examples):\n            out = tokenizer(examples[\"text\"], truncation=True, max_length=max_len, padding=\"max_length\", return_tensors=None)\n            pad_id = tokenizer.pad_token_id\n            out[\"labels\"] = [[x if x != pad_id else -100 for x in seq] for seq in out[\"input_ids\"]]\n            return out\n        train_tokenized = train_ds.map(_tokenize, batched=True, remove_columns=train_ds.column_names)\n        test_tokenized = test_ds.map(_tokenize, batched=True, remove_columns=test_ds.column_names)\n        train_tokenized.set_format(\"torch\")\n        test_tokenized.set_format(\"torch\")\n        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n        print(\"Loaded train/test from train.jsonl and test.jsonl\")\n    if hasattr(sys.stdout, \"reconfigure\"):\n        sys.stdout.reconfigure(line_buffering=True)\n    torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        gpu_mem_before = torch.cuda.memory_allocated(0) / 1024**3\n    else:\n        gpu_mem_before = 0\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\n    lora_config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM,\n    )\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n\n    args = TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=per_device_eval_batch_size,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        num_train_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=0.01,\n        warmup_steps=max(1, int(0.03 * len(train_tokenized) // (per_device_train_batch_size * gradient_accumulation_steps))),\n        logging_steps=10,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        fp16=True,\n        report_to=\"none\",\n        log_level=\"info\",\n        disable_tqdm=False,\n    )\n\n    steps_per_epoch = max(1, len(train_tokenized) // (per_device_train_batch_size * gradient_accumulation_steps))\n    total_steps = steps_per_epoch * num_epochs\n    progress_cb = ProgressCallback(steps_per_epoch, total_steps)\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_tokenized,\n        eval_dataset=test_tokenized,\n        data_collator=data_collator,\n        callbacks=[progress_cb],\n    )\n\n    print(f\"\\n{'='*60}\\nStarting training: {run_id}\")\n    print(f\"Epochs: {num_epochs} | Effective batch size: {per_device_train_batch_size * gradient_accumulation_steps}\")\n    print(f\"Total steps per epoch: ~{steps_per_epoch}\")\n    print(f\"{'='*60}\\n\")\n\n    start = time.time()\n    train_result = trainer.train()\n    elapsed = time.time() - start\n\n    print(f\"\\n{'='*60}\\nTraining completed!\")\n    print(f\"Total time: {elapsed/60:.2f} minutes ({elapsed:.1f} seconds)\")\n    print(f\"Steps completed: {train_result.global_step}\")\n    print(f\"Final training loss: {train_result.training_loss:.4f}\")\n    print(f\"{'='*60}\\n\")\n\n    if torch.cuda.is_available():\n        gpu_mem_after = torch.cuda.memory_allocated(0) / 1024**3\n    else:\n        gpu_mem_after = 0\n\n    eval_metrics = trainer.evaluate()\n    eval_loss = eval_metrics.get(\"eval_loss\", float(\"nan\"))\n\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n    experiment_log.append({\n        \"run_id\": run_id,\n        \"learning_rate\": learning_rate,\n        \"batch_size\": per_device_train_batch_size,\n        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n        \"effective_batch\": per_device_train_batch_size * gradient_accumulation_steps,\n        \"epochs\": num_epochs,\n        \"lora_r\": lora_r,\n        \"optimizer\": \"AdamW\",\n        \"eval_loss\": round(eval_loss, 4),\n        \"train_loss\": round(train_result.training_loss, 4),\n        \"training_time_min\": round(elapsed / 60, 2),\n        \"gpu_mem_peak_gb\": round(gpu_mem_after, 2),\n    })\n    print(f\"Run {run_id} done. Eval loss: {eval_loss:.4f} | Time: {elapsed/60:.2f} min | GPU mem: {gpu_mem_after:.2f} GB\")\n    del model, trainer\n    torch.cuda.empty_cache()\n    return eval_loss","metadata":{"id":"UGTh1_QvOKRC","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:47.879435Z","iopub.execute_input":"2026-02-26T02:27:47.879727Z","iopub.status.idle":"2026-02-26T02:27:47.900750Z","shell.execute_reply.started":"2026-02-26T02:27:47.879703Z","shell.execute_reply":"2026-02-26T02:27:47.899912Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Hyperparameter tuning (learning rate 1e-4–5e-5, batch size 2–4, epochs 1–3)\n\nWe run **three experiments** to compare learning rate and batch size. Optimizer: **AdamW** (Trainer default). Each run records GPU memory (peak), training time, and eval loss for the experiment table.","metadata":{"id":"td1M01VnOKRC"}},{"cell_type":"code","source":"# Run 1 (baseline): learning rate 5e-5, batch size 2, gradient accumulation 4, 2 epochs\nrun_training(\"run1_baseline\", learning_rate=5e-5, per_device_train_batch_size=2,\n             gradient_accumulation_steps=4, num_epochs=2, output_dir=\"./tinyllama_lora_run1\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7af36bc19fcc407b89614c63db8620ff","89f100b5db60490fa54a13b6fa4b6384","6e42623483504182a018e47ffac847e1","a5e8f19f9d6b420fb57666326ca90bc7","6418b69291564023b05fe538fdf95072","d70b5240ccc842e9a1cbbf8b68c347a9","6f75d4654afa4c7aad053c4edfd6eb9c","4ae94e62569e485880c13f3e2d9d00e4","f6be2dabb4414b93b5810edf167da1db","7db1d568e6644a299e4615bb02796850","3f5644397f944c39b902d04a1178259f","b036abfa032f4998b3b051818a1937ec","9378f05b484d4aef80fa937be3bfd019","bbf654e99cf648f69d0defdc7ace050a","c5bf6f6c6be3404391623dc79c242429","526163674ec947e395f0c99e99675bd7","ec294483c3c5423cb0abe4d88b73e879","fddde1a91d15490886a7a623b93cd1e7","c30e87fc9b14407a9d3ec904558d3e5d","2ffac522534c4bd7802172039931a69a","ca4bea7005ec455eb9707d37f11089b5","c622ac92e7ad4a71801da411c8241141","c1f63270156240ecbecefbcc7d918142","4c26d5155f964060ae302773ef3ecd2e","df5834b542714609b173e28d257f51fc","2213210940864652a5bb561c72f82f39","1d307e2e5ec74bf983942c1b6c0aad43","c5a585dfabaf473b8864149f7fac0a9b","6d69988f6a6e4893be09cc2b07135439","abad6df14f33408384e7826884d3e29f","ea2d1127bd7f48198355e965ac7b05cb","686f109f52134640b59df59d30a4f381","6a9c4ec3e35b474593545c3b0f3b96e7"]},"id":"A70F_O4GVsHZ","outputId":"a4e1073e-cfe4-4988-87f3-09f5042451d7","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:27:47.901653Z","iopub.execute_input":"2026-02-26T02:27:47.902325Z","iopub.status.idle":"2026-02-26T02:46:09.708155Z","shell.execute_reply.started":"2026-02-26T02:27:47.902297Z","shell.execute_reply":"2026-02-26T02:46:09.707497Z"}},"outputs":[{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b0e4621fdb42e6b36d8bf3943d2981"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66e0e7d09e242348fffd8a2b3243303"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83da2a5790ab4b9fb3da5fae23081c37"}},"metadata":{}},{"name":"stdout","text":"trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n\n============================================================\nStarting training: run1_baseline\nEpochs: 2 | Effective batch size: 8\nTotal steps per epoch: ~342\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"***** Running training *****\n  Num examples = 2,736\n  Num Epochs = 2\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 684\n  Number of trainable parameters = 2,252,800\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 1 ---\n  Step 1/684 (0.1%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='684' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [684/684 17:40, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.344859</td>\n      <td>1.366063</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.343672</td>\n      <td>1.358983</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"  Step 5/684 (0.7%)\n  Step 10/684 (1.5%)\n  [Step 10] loss=2.3249  lr=4.50e-05\n  Step 15/684 (2.2%)\n  Step 20/684 (2.9%)\n  [Step 20] loss=2.1900  lr=4.93e-05\n  Step 25/684 (3.7%)\n  Step 30/684 (4.4%)\n  [Step 30] loss=2.0590  lr=4.86e-05\n  Step 35/684 (5.1%)\n  Step 40/684 (5.8%)\n  [Step 40] loss=1.9313  lr=4.78e-05\n  Step 45/684 (6.6%)\n  Step 50/684 (7.3%)\n  [Step 50] loss=1.7229  lr=4.71e-05\n  Step 55/684 (8.0%)\n  Step 60/684 (8.8%)\n  [Step 60] loss=1.5533  lr=4.64e-05\n  Step 65/684 (9.5%)\n  Step 70/684 (10.2%)\n  [Step 70] loss=1.4076  lr=4.56e-05\n  Step 75/684 (11.0%)\n  Step 80/684 (11.7%)\n  [Step 80] loss=1.4150  lr=4.49e-05\n  Step 85/684 (12.4%)\n  Step 90/684 (13.2%)\n  [Step 90] loss=1.4461  lr=4.41e-05\n  Step 95/684 (13.9%)\n  Step 100/684 (14.6%)\n  [Step 100] loss=1.3702  lr=4.34e-05\n  Step 105/684 (15.4%)\n  Step 110/684 (16.1%)\n  [Step 110] loss=1.3986  lr=4.27e-05\n  Step 115/684 (16.8%)\n  Step 120/684 (17.5%)\n  [Step 120] loss=1.3760  lr=4.19e-05\n  Step 125/684 (18.3%)\n  Step 130/684 (19.0%)\n  [Step 130] loss=1.4187  lr=4.12e-05\n  Step 135/684 (19.7%)\n  Step 140/684 (20.5%)\n  [Step 140] loss=1.3801  lr=4.04e-05\n  Step 145/684 (21.2%)\n  Step 150/684 (21.9%)\n  [Step 150] loss=1.4057  lr=3.97e-05\n  Step 155/684 (22.7%)\n  Step 160/684 (23.4%)\n  [Step 160] loss=1.4165  lr=3.89e-05\n  Step 165/684 (24.1%)\n  Step 170/684 (24.9%)\n  [Step 170] loss=1.3985  lr=3.82e-05\n  Step 175/684 (25.6%)\n  Step 180/684 (26.3%)\n  [Step 180] loss=1.4006  lr=3.75e-05\n  Step 185/684 (27.0%)\n  Step 190/684 (27.8%)\n  [Step 190] loss=1.4340  lr=3.67e-05\n  Step 195/684 (28.5%)\n  Step 200/684 (29.2%)\n  [Step 200] loss=1.3999  lr=3.60e-05\n  Step 205/684 (30.0%)\n  Step 210/684 (30.7%)\n  [Step 210] loss=1.3688  lr=3.52e-05\n  Step 215/684 (31.4%)\n  Step 220/684 (32.2%)\n  [Step 220] loss=1.3676  lr=3.45e-05\n  Step 225/684 (32.9%)\n  Step 230/684 (33.6%)\n  [Step 230] loss=1.4077  lr=3.38e-05\n  Step 235/684 (34.4%)\n  Step 240/684 (35.1%)\n  [Step 240] loss=1.4070  lr=3.30e-05\n  Step 245/684 (35.8%)\n  Step 250/684 (36.5%)\n  [Step 250] loss=1.3609  lr=3.23e-05\n  Step 255/684 (37.3%)\n  Step 260/684 (38.0%)\n  [Step 260] loss=1.3864  lr=3.15e-05\n  Step 265/684 (38.7%)\n  Step 270/684 (39.5%)\n  [Step 270] loss=1.3671  lr=3.08e-05\n  Step 275/684 (40.2%)\n  Step 280/684 (40.9%)\n  [Step 280] loss=1.4101  lr=3.00e-05\n  Step 285/684 (41.7%)\n  Step 290/684 (42.4%)\n  [Step 290] loss=1.3940  lr=2.93e-05\n  Step 295/684 (43.1%)\n  Step 300/684 (43.9%)\n  [Step 300] loss=1.3754  lr=2.86e-05\n  Step 305/684 (44.6%)\n  Step 310/684 (45.3%)\n  [Step 310] loss=1.3985  lr=2.78e-05\n  Step 315/684 (46.1%)\n  Step 320/684 (46.8%)\n  [Step 320] loss=1.3690  lr=2.71e-05\n  Step 325/684 (47.5%)\n  Step 330/684 (48.2%)\n  [Step 330] loss=1.3842  lr=2.63e-05\n  Step 335/684 (49.0%)\n  Step 340/684 (49.7%)\n  [Step 340] loss=1.3449  lr=2.56e-05\n","output_type":"stream"},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 305\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"  [Eval] eval_loss=1.3661\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./tinyllama_lora_run1/checkpoint-342\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in ./tinyllama_lora_run1/checkpoint-342/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run1/checkpoint-342/tokenizer_config.json\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 2 ---\n  Step 345/684 (50.4%)\n  Step 350/684 (51.2%)\n  [Step 350] loss=1.3955  lr=2.49e-05\n  Step 355/684 (51.9%)\n  Step 360/684 (52.6%)\n  [Step 360] loss=1.4038  lr=2.41e-05\n  Step 365/684 (53.4%)\n  Step 370/684 (54.1%)\n  [Step 370] loss=1.3865  lr=2.34e-05\n  Step 375/684 (54.8%)\n  Step 380/684 (55.6%)\n  [Step 380] loss=1.3831  lr=2.26e-05\n  Step 385/684 (56.3%)\n  Step 390/684 (57.0%)\n  [Step 390] loss=1.3601  lr=2.19e-05\n  Step 395/684 (57.7%)\n  Step 400/684 (58.5%)\n  [Step 400] loss=1.3539  lr=2.11e-05\n  Step 405/684 (59.2%)\n  Step 410/684 (59.9%)\n  [Step 410] loss=1.3856  lr=2.04e-05\n  Step 415/684 (60.7%)\n  Step 420/684 (61.4%)\n  [Step 420] loss=1.3722  lr=1.97e-05\n  Step 425/684 (62.1%)\n  Step 430/684 (62.9%)\n  [Step 430] loss=1.4076  lr=1.89e-05\n  Step 435/684 (63.6%)\n  Step 440/684 (64.3%)\n  [Step 440] loss=1.3741  lr=1.82e-05\n  Step 445/684 (65.1%)\n  Step 450/684 (65.8%)\n  [Step 450] loss=1.3807  lr=1.74e-05\n  Step 455/684 (66.5%)\n  Step 460/684 (67.3%)\n  [Step 460] loss=1.3713  lr=1.67e-05\n  Step 465/684 (68.0%)\n  Step 470/684 (68.7%)\n  [Step 470] loss=1.3623  lr=1.59e-05\n  Step 475/684 (69.4%)\n  Step 480/684 (70.2%)\n  [Step 480] loss=1.3605  lr=1.52e-05\n  Step 485/684 (70.9%)\n  Step 490/684 (71.6%)\n  [Step 490] loss=1.3271  lr=1.45e-05\n  Step 495/684 (72.4%)\n  Step 500/684 (73.1%)\n  [Step 500] loss=1.3592  lr=1.37e-05\n  Step 505/684 (73.8%)\n  Step 510/684 (74.6%)\n  [Step 510] loss=1.3725  lr=1.30e-05\n  Step 515/684 (75.3%)\n  Step 520/684 (76.0%)\n  [Step 520] loss=1.3718  lr=1.22e-05\n  Step 525/684 (76.8%)\n  Step 530/684 (77.5%)\n  [Step 530] loss=1.3382  lr=1.15e-05\n  Step 535/684 (78.2%)\n  Step 540/684 (78.9%)\n  [Step 540] loss=1.2997  lr=1.08e-05\n  Step 545/684 (79.7%)\n  Step 550/684 (80.4%)\n  [Step 550] loss=1.3860  lr=1.00e-05\n  Step 555/684 (81.1%)\n  Step 560/684 (81.9%)\n  [Step 560] loss=1.3838  lr=9.27e-06\n  Step 565/684 (82.6%)\n  Step 570/684 (83.3%)\n  [Step 570] loss=1.3446  lr=8.53e-06\n  Step 575/684 (84.1%)\n  Step 580/684 (84.8%)\n  [Step 580] loss=1.3835  lr=7.79e-06\n  Step 585/684 (85.5%)\n  Step 590/684 (86.3%)\n  [Step 590] loss=1.3496  lr=7.05e-06\n  Step 595/684 (87.0%)\n  Step 600/684 (87.7%)\n  [Step 600] loss=1.3416  lr=6.31e-06\n  Step 605/684 (88.5%)\n  Step 610/684 (89.2%)\n  [Step 610] loss=1.3782  lr=5.56e-06\n  Step 615/684 (89.9%)\n  Step 620/684 (90.6%)\n  [Step 620] loss=1.3318  lr=4.82e-06\n  Step 625/684 (91.4%)\n  Step 630/684 (92.1%)\n  [Step 630] loss=1.4263  lr=4.08e-06\n  Step 635/684 (92.8%)\n  Step 640/684 (93.6%)\n  [Step 640] loss=1.3526  lr=3.34e-06\n  Step 645/684 (94.3%)\n  Step 650/684 (95.0%)\n  [Step 650] loss=1.4231  lr=2.60e-06\n  Step 655/684 (95.8%)\n  Step 660/684 (96.5%)\n  [Step 660] loss=1.3899  lr=1.85e-06\n  Step 665/684 (97.2%)\n  Step 670/684 (98.0%)\n  [Step 670] loss=1.3800  lr=1.11e-06\n  Step 675/684 (98.7%)\n  Step 680/684 (99.4%)\n  [Step 680] loss=1.3437  lr=3.71e-07\n","output_type":"stream"},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 305\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"  [Eval] eval_loss=1.3590\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./tinyllama_lora_run1/checkpoint-684\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in ./tinyllama_lora_run1/checkpoint-684/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run1/checkpoint-684/tokenizer_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./tinyllama_lora_run1/checkpoint-684 (score: 1.3589831590652466).\n","output_type":"stream"},{"name":"stdout","text":"  [Step 684] {'train_runtime': 1063.6976, 'train_samples_per_second': 5.144, 'train_steps_per_second': 0.643, 'total_flos': 1.7428006154797056e+16, 'train_loss': 1.4319124891046893, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 305\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nTraining completed!\nTotal time: 17.73 minutes (1064.1 seconds)\nSteps completed: 684\nFinal training loss: 1.4319\n============================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [153/153 00:22]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"  [Eval] eval_loss=1.3590\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./tinyllama_lora_run1\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in ./tinyllama_lora_run1/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run1/tokenizer_config.json\nchat template saved in ./tinyllama_lora_run1/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run1/tokenizer_config.json\n","output_type":"stream"},{"name":"stdout","text":"Run run1_baseline done. Eval loss: 1.3590 | Time: 17.73 min | GPU mem: 0.97 GB\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"1.3589831590652466"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Run 2 (tuned): higher learning rate 1e-4, same batch/accum, 1 epoch (faster; compare eval loss)\nrun_training(\"run2_lr1e4\", learning_rate=1e-4, per_device_train_batch_size=2,\n             gradient_accumulation_steps=4, num_epochs=1, output_dir=\"./tinyllama_lora_run2\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5dcc466c37554071b39599448a687fe1","a3304b565c724af18a81e4538b72a88c","ae9552ad26034e3395f7625d301ece64","820141481e5d48fc89b7bffb1bbed73c","8c4303aa12da47f2a0c5e573e95779bd","4707393f57a04cbf81bb608cd01950fa","f090ba64a9c44147b5a6c87661c4ac3b","416acbe0d87b4193bd69dcbe21374541","ff8a6d6ab8094f1da8bc6ca360ec1e6c","bb52b2e9d52844dd8156e55347583bea","3063b6f228f448db9b4b1b9a3e6e8ec6"]},"id":"P90UfJ4fOKRC","outputId":"9812bf2f-56c6-48e0-ff05-6fe313baf9e6","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:46:09.709318Z","iopub.execute_input":"2026-02-26T02:46:09.710462Z","iopub.status.idle":"2026-02-26T02:55:32.461657Z","shell.execute_reply.started":"2026-02-26T02:46:09.710426Z","shell.execute_reply":"2026-02-26T02:55:32.460910Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"float16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"use_cache\": true\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a414de065ec140cdafe623c84a924df2"}},"metadata":{}},{"name":"stderr","text":"loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"max_length\": 2048,\n  \"pad_token_id\": 0\n}\n\nCould not locate the custom_generate/generate.py inside TinyLlama/TinyLlama-1.1B-Chat-v1.0.\nPyTorch: setting up devices\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n\n============================================================\nStarting training: run2_lr1e4\nEpochs: 1 | Effective batch size: 8\nTotal steps per epoch: ~342\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"***** Running training *****\n  Num examples = 2,736\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 342\n  Number of trainable parameters = 2,252,800\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 1 ---\n  Step 1/342 (0.3%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [342/342 08:55, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.340160</td>\n      <td>1.361849</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"  Step 5/342 (1.5%)\n  Step 10/342 (2.9%)\n  [Step 10] loss=2.3123  lr=9.00e-05\n  Step 15/342 (4.4%)\n  Step 20/342 (5.8%)\n  [Step 20] loss=2.0717  lr=9.73e-05\n  Step 25/342 (7.3%)\n  Step 30/342 (8.8%)\n  [Step 30] loss=1.8284  lr=9.43e-05\n  Step 35/342 (10.2%)\n  Step 40/342 (11.7%)\n  [Step 40] loss=1.5745  lr=9.13e-05\n  Step 45/342 (13.2%)\n  Step 50/342 (14.6%)\n  [Step 50] loss=1.4216  lr=8.83e-05\n  Step 55/342 (16.1%)\n  Step 60/342 (17.5%)\n  [Step 60] loss=1.4207  lr=8.52e-05\n  Step 65/342 (19.0%)\n  Step 70/342 (20.5%)\n  [Step 70] loss=1.3639  lr=8.22e-05\n  Step 75/342 (21.9%)\n  Step 80/342 (23.4%)\n  [Step 80] loss=1.3874  lr=7.92e-05\n  Step 85/342 (24.9%)\n  Step 90/342 (26.3%)\n  [Step 90] loss=1.4271  lr=7.62e-05\n  Step 95/342 (27.8%)\n  Step 100/342 (29.2%)\n  [Step 100] loss=1.3555  lr=7.32e-05\n  Step 105/342 (30.7%)\n  Step 110/342 (32.2%)\n  [Step 110] loss=1.3833  lr=7.02e-05\n  Step 115/342 (33.6%)\n  Step 120/342 (35.1%)\n  [Step 120] loss=1.3603  lr=6.72e-05\n  Step 125/342 (36.5%)\n  Step 130/342 (38.0%)\n  [Step 130] loss=1.4050  lr=6.42e-05\n  Step 135/342 (39.5%)\n  Step 140/342 (40.9%)\n  [Step 140] loss=1.3666  lr=6.11e-05\n  Step 145/342 (42.4%)\n  Step 150/342 (43.9%)\n  [Step 150] loss=1.3932  lr=5.81e-05\n  Step 155/342 (45.3%)\n  Step 160/342 (46.8%)\n  [Step 160] loss=1.4040  lr=5.51e-05\n  Step 165/342 (48.2%)\n  Step 170/342 (49.7%)\n  [Step 170] loss=1.3866  lr=5.21e-05\n  Step 175/342 (51.2%)\n  Step 180/342 (52.6%)\n  [Step 180] loss=1.3900  lr=4.91e-05\n  Step 185/342 (54.1%)\n  Step 190/342 (55.6%)\n  [Step 190] loss=1.4231  lr=4.61e-05\n  Step 195/342 (57.0%)\n  Step 200/342 (58.5%)\n  [Step 200] loss=1.3896  lr=4.31e-05\n  Step 205/342 (59.9%)\n  Step 210/342 (61.4%)\n  [Step 210] loss=1.3593  lr=4.01e-05\n  Step 215/342 (62.9%)\n  Step 220/342 (64.3%)\n  [Step 220] loss=1.3570  lr=3.70e-05\n  Step 225/342 (65.8%)\n  Step 230/342 (67.3%)\n  [Step 230] loss=1.3972  lr=3.40e-05\n  Step 235/342 (68.7%)\n  Step 240/342 (70.2%)\n  [Step 240] loss=1.3978  lr=3.10e-05\n  Step 245/342 (71.6%)\n  Step 250/342 (73.1%)\n  [Step 250] loss=1.3518  lr=2.80e-05\n  Step 255/342 (74.6%)\n  Step 260/342 (76.0%)\n  [Step 260] loss=1.3787  lr=2.50e-05\n  Step 265/342 (77.5%)\n  Step 270/342 (78.9%)\n  [Step 270] loss=1.3608  lr=2.20e-05\n  Step 275/342 (80.4%)\n  Step 280/342 (81.9%)\n  [Step 280] loss=1.4020  lr=1.90e-05\n  Step 285/342 (83.3%)\n  Step 290/342 (84.8%)\n  [Step 290] loss=1.3868  lr=1.60e-05\n  Step 295/342 (86.3%)\n  Step 300/342 (87.7%)\n  [Step 300] loss=1.3680  lr=1.30e-05\n  Step 305/342 (89.2%)\n  Step 310/342 (90.6%)\n  [Step 310] loss=1.3916  lr=9.94e-06\n  Step 315/342 (92.1%)\n  Step 320/342 (93.6%)\n  [Step 320] loss=1.3615  lr=6.93e-06\n  Step 325/342 (95.0%)\n  Step 330/342 (96.5%)\n  [Step 330] loss=1.3777  lr=3.92e-06\n  Step 335/342 (98.0%)\n  Step 340/342 (99.4%)\n  [Step 340] loss=1.3402  lr=9.04e-07\n","output_type":"stream"},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 305\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"  [Eval] eval_loss=1.3618\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./tinyllama_lora_run2/checkpoint-342\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in ./tinyllama_lora_run2/checkpoint-342/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run2/checkpoint-342/tokenizer_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./tinyllama_lora_run2/checkpoint-342 (score: 1.361849069595337).\n","output_type":"stream"},{"name":"stdout","text":"  [Step 342] {'train_runtime': 537.1511, 'train_samples_per_second': 5.094, 'train_steps_per_second': 0.637, 'total_flos': 8714003077398528.0, 'train_loss': 1.4499629359496267, 'epoch': 1.0}\n","output_type":"stream"},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 305\n  Batch size = 2\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nTraining completed!\nTotal time: 8.96 minutes (537.5 seconds)\nSteps completed: 342\nFinal training loss: 1.4500\n============================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [153/153 00:22]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"  [Eval] eval_loss=1.3618\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./tinyllama_lora_run2\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in ./tinyllama_lora_run2/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run2/tokenizer_config.json\nchat template saved in ./tinyllama_lora_run2/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run2/tokenizer_config.json\n","output_type":"stream"},{"name":"stdout","text":"Run run2_lr1e4 done. Eval loss: 1.3618 | Time: 8.96 min | GPU mem: 1.05 GB\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"1.361849069595337"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Run 3 (tuned): batch size 4, fewer gradient accumulation steps; lr 5e-5, 2 epochs\nrun_training(\"run3_batch4\", learning_rate=5e-5, per_device_train_batch_size=4,\n             gradient_accumulation_steps=2, num_epochs=2, output_dir=\"./tinyllama_lora_run3\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c75479434a4a4a2cb4a6d066ed5b5da1","79b688732d5744199eb62b4e89275024","5dc51b199eea45ff8de35150ab4af249","fe036b3ee73a4cb1bc5e103f9bcaa161","56210125fb46446780cd9a6af1e0587e","cf5a0c8fb1c4497bb5891a187beb9c21","0ca3eaf6c7b34c028f0b47f606f74c7f","71b4c8c10c9642228e675e8c81a208bf","be1833aeb99f4b0995831d9751c6fec1","ba6caedbaa964a4abba74f1de832efb5","fe845cac9beb484489ed9923d0955856"]},"id":"fDLbCdl8OKRD","outputId":"91a8be70-05ad-454a-c653-8a6e224e3174","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T02:55:32.462753Z","iopub.execute_input":"2026-02-26T02:55:32.463054Z","iopub.status.idle":"2026-02-26T03:11:58.226425Z","shell.execute_reply.started":"2026-02-26T02:55:32.463022Z","shell.execute_reply":"2026-02-26T03:11:58.225712Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"float16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"use_cache\": true\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4003aedad54ff0ac732e4c3f6da218"}},"metadata":{}},{"name":"stderr","text":"loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"max_length\": 2048,\n  \"pad_token_id\": 0\n}\n\nCould not locate the custom_generate/generate.py inside TinyLlama/TinyLlama-1.1B-Chat-v1.0.\nPyTorch: setting up devices\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n\n============================================================\nStarting training: run3_batch4\nEpochs: 2 | Effective batch size: 8\nTotal steps per epoch: ~342\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"***** Running training *****\n  Num examples = 2,736\n  Num Epochs = 2\n  Instantaneous batch size per device = 4\n  Training with DataParallel so batch size has been adjusted to: 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 2\n  Total optimization steps = 342\n  Number of trainable parameters = 2,252,800\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 1 ---\n  Step 1/684 (0.1%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [342/342 15:48, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.379826</td>\n      <td>1.398059</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.379885</td>\n      <td>1.388920</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"  Step 5/684 (0.7%)\n  Step 10/684 (1.5%)\n  [Step 10] loss=2.2995  lr=4.50e-05\n  Step 15/684 (2.2%)\n  Step 20/684 (2.9%)\n  [Step 20] loss=2.1948  lr=4.86e-05\n  Step 25/684 (3.7%)\n  Step 30/684 (4.4%)\n  [Step 30] loss=2.0574  lr=4.71e-05\n  Step 35/684 (5.1%)\n  Step 40/684 (5.8%)\n  [Step 40] loss=1.8892  lr=4.56e-05\n  Step 45/684 (6.6%)\n  Step 50/684 (7.3%)\n  [Step 50] loss=1.6960  lr=4.41e-05\n  Step 55/684 (8.0%)\n  Step 60/684 (8.8%)\n  [Step 60] loss=1.5059  lr=4.26e-05\n  Step 65/684 (9.5%)\n  Step 70/684 (10.2%)\n  [Step 70] loss=1.4475  lr=4.11e-05\n  Step 75/684 (11.0%)\n  Step 80/684 (11.7%)\n  [Step 80] loss=1.4433  lr=3.96e-05\n  Step 85/684 (12.4%)\n  Step 90/684 (13.2%)\n  [Step 90] loss=1.4295  lr=3.81e-05\n  Step 95/684 (13.9%)\n  Step 100/684 (14.6%)\n  [Step 100] loss=1.4377  lr=3.66e-05\n  Step 105/684 (15.4%)\n  Step 110/684 (16.1%)\n  [Step 110] loss=1.3876  lr=3.51e-05\n  Step 115/684 (16.8%)\n  Step 120/684 (17.5%)\n  [Step 120] loss=1.4288  lr=3.36e-05\n  Step 125/684 (18.3%)\n  Step 130/684 (19.0%)\n  [Step 130] loss=1.3927  lr=3.21e-05\n  Step 135/684 (19.7%)\n  Step 140/684 (20.5%)\n  [Step 140] loss=1.4077  lr=3.06e-05\n  Step 145/684 (21.2%)\n  Step 150/684 (21.9%)\n  [Step 150] loss=1.4012  lr=2.91e-05\n  Step 155/684 (22.7%)\n  Step 160/684 (23.4%)\n  [Step 160] loss=1.4028  lr=2.76e-05\n  Step 165/684 (24.1%)\n  Step 170/684 (24.9%)\n  [Step 170] loss=1.3798  lr=2.61e-05\n","output_type":"stream"},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 305\n  Batch size = 4\n","output_type":"stream"},{"name":"stdout","text":"  [Eval] eval_loss=1.3981\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./tinyllama_lora_run3/checkpoint-171\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in ./tinyllama_lora_run3/checkpoint-171/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run3/checkpoint-171/tokenizer_config.json\n","output_type":"stream"},{"name":"stdout","text":"\n--- Epoch 2 ---\n  Step 175/684 (25.6%)\n  Step 180/684 (26.3%)\n  [Step 180] loss=1.4192  lr=2.45e-05\n  Step 185/684 (27.0%)\n  Step 190/684 (27.8%)\n  [Step 190] loss=1.4045  lr=2.30e-05\n  Step 215/684 (31.4%)\n  Step 220/684 (32.2%)\n  [Step 220] loss=1.4075  lr=1.85e-05\n  Step 225/684 (32.9%)\n  Step 230/684 (33.6%)\n  [Step 230] loss=1.4012  lr=1.70e-05\n  Step 235/684 (34.4%)\n  Step 240/684 (35.1%)\n  [Step 240] loss=1.3801  lr=1.55e-05\n  Step 245/684 (35.8%)\n  Step 250/684 (36.5%)\n  [Step 250] loss=1.3650  lr=1.40e-05\n  Step 255/684 (37.3%)\n  Step 260/684 (38.0%)\n  [Step 260] loss=1.3921  lr=1.25e-05\n  Step 265/684 (38.7%)\n  Step 270/684 (39.5%)\n  [Step 270] loss=1.3349  lr=1.10e-05\n  Step 275/684 (40.2%)\n  Step 280/684 (40.9%)\n  [Step 280] loss=1.4058  lr=9.49e-06\n  Step 285/684 (41.7%)\n  Step 290/684 (42.4%)\n  [Step 290] loss=1.3848  lr=7.98e-06\n  Step 295/684 (43.1%)\n  Step 300/684 (43.9%)\n  [Step 300] loss=1.3632  lr=6.48e-06\n  Step 305/684 (44.6%)\n  Step 310/684 (45.3%)\n  [Step 310] loss=1.3716  lr=4.97e-06\n  Step 315/684 (46.1%)\n  Step 320/684 (46.8%)\n  [Step 320] loss=1.4054  lr=3.46e-06\n  Step 325/684 (47.5%)\n  Step 330/684 (48.2%)\n  [Step 330] loss=1.4257  lr=1.96e-06\n  Step 335/684 (49.0%)\n  Step 340/684 (49.7%)\n  [Step 340] loss=1.3799  lr=4.52e-07\n","output_type":"stream"},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 305\n  Batch size = 4\n","output_type":"stream"},{"name":"stdout","text":"  [Eval] eval_loss=1.3889\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./tinyllama_lora_run3/checkpoint-342\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in ./tinyllama_lora_run3/checkpoint-342/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run3/checkpoint-342/tokenizer_config.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./tinyllama_lora_run3/checkpoint-342 (score: 1.3889195919036865).\n","output_type":"stream"},{"name":"stdout","text":"  [Step 342] {'train_runtime': 951.5416, 'train_samples_per_second': 5.751, 'train_steps_per_second': 0.359, 'total_flos': 1.7428006154797056e+16, 'train_loss': 1.4943211580577649, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 305\n  Batch size = 4\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nTraining completed!\nTotal time: 15.87 minutes (951.9 seconds)\nSteps completed: 342\nFinal training loss: 1.4943\n============================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [77/77 00:30]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"  [Eval] eval_loss=1.3889\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to ./tinyllama_lora_run3\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"bfloat16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nSaving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\nchat template saved in ./tinyllama_lora_run3/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run3/tokenizer_config.json\nchat template saved in ./tinyllama_lora_run3/chat_template.jinja\ntokenizer config file saved in ./tinyllama_lora_run3/tokenizer_config.json\n","output_type":"stream"},{"name":"stdout","text":"Run run3_batch4 done. Eval loss: 1.3889 | Time: 15.87 min | GPU mem: 2.10 GB\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"1.3889195919036865"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### Experiment table (rubric: document hyperparameters and validation metrics)\n\nThe table below compares different hyperparameter settings. **Validation metric**: eval loss (lower is better). **Improvement over baseline**: percentage change in eval loss relative to run1_baseline.","metadata":{"id":"QwE-7IlcOKRD"}},{"cell_type":"code","source":"# Build experiment table and compute improvement over baseline\nexperiment_df = pd.DataFrame(experiment_log)\nif len(experiment_df) > 0:\n    baseline_loss = experiment_df[experiment_df[\"run_id\"] == \"run1_baseline\"][\"eval_loss\"].values\n    baseline_loss = baseline_loss[0] if len(baseline_loss) else experiment_df[\"eval_loss\"].iloc[0]\n    experiment_df[\"improvement_vs_baseline_%\"] = (\n        (baseline_loss - experiment_df[\"eval_loss\"]) / baseline_loss * 100\n    ).round(2)\ndisplay(experiment_df)\n\n# Save experiment log to a file so you can open it (e.g. in project folder or Colab files)\nif len(experiment_df) > 0:\n    experiment_df.to_csv(\"experiment_log.csv\", index=False)\n    print(\"Experiment log saved to: experiment_log.csv\")\n\n# Summary for rubric: multiple hyperparameters tuned, results vs baseline\nif len(experiment_df) >= 2:\n    best_run = experiment_df.loc[experiment_df[\"eval_loss\"].idxmin()]\n    print(f\"\\nBest run: {best_run['run_id']} (eval_loss={best_run['eval_loss']})\")\n    print(f\"Improvement over baseline: {best_run['improvement_vs_baseline_%']}%\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"xJmj6QyQOKRD","outputId":"16136031-f15b-454a-a9ab-90452aa7eed0","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T03:11:58.227358Z","iopub.execute_input":"2026-02-26T03:11:58.227633Z","iopub.status.idle":"2026-02-26T03:11:58.257242Z","shell.execute_reply.started":"2026-02-26T03:11:58.227597Z","shell.execute_reply":"2026-02-26T03:11:58.256492Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"          run_id  learning_rate  batch_size  gradient_accumulation_steps  \\\n0  run1_baseline        0.00005           2                            4   \n1     run2_lr1e4        0.00010           2                            4   \n2    run3_batch4        0.00005           4                            2   \n\n   effective_batch  epochs  lora_r optimizer  eval_loss  train_loss  \\\n0                8       2       8     AdamW     1.3590      1.4319   \n1                8       1       8     AdamW     1.3618      1.4500   \n2                8       2       8     AdamW     1.3889      1.4943   \n\n   training_time_min  gpu_mem_peak_gb  improvement_vs_baseline_%  \n0              17.73             0.97                       0.00  \n1               8.96             1.05                      -0.21  \n2              15.87             2.10                      -2.20  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_id</th>\n      <th>learning_rate</th>\n      <th>batch_size</th>\n      <th>gradient_accumulation_steps</th>\n      <th>effective_batch</th>\n      <th>epochs</th>\n      <th>lora_r</th>\n      <th>optimizer</th>\n      <th>eval_loss</th>\n      <th>train_loss</th>\n      <th>training_time_min</th>\n      <th>gpu_mem_peak_gb</th>\n      <th>improvement_vs_baseline_%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>run1_baseline</td>\n      <td>0.00005</td>\n      <td>2</td>\n      <td>4</td>\n      <td>8</td>\n      <td>2</td>\n      <td>8</td>\n      <td>AdamW</td>\n      <td>1.3590</td>\n      <td>1.4319</td>\n      <td>17.73</td>\n      <td>0.97</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>run2_lr1e4</td>\n      <td>0.00010</td>\n      <td>2</td>\n      <td>4</td>\n      <td>8</td>\n      <td>1</td>\n      <td>8</td>\n      <td>AdamW</td>\n      <td>1.3618</td>\n      <td>1.4500</td>\n      <td>8.96</td>\n      <td>1.05</td>\n      <td>-0.21</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>run3_batch4</td>\n      <td>0.00005</td>\n      <td>4</td>\n      <td>2</td>\n      <td>8</td>\n      <td>2</td>\n      <td>8</td>\n      <td>AdamW</td>\n      <td>1.3889</td>\n      <td>1.4943</td>\n      <td>15.87</td>\n      <td>2.10</td>\n      <td>-2.20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Experiment log saved to: experiment_log.csv\n\nBest run: run1_baseline (eval_loss=1.359)\nImprovement over baseline: 0.0%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Summary: Baseline vs Run2 vs Run3\n\n- **Run1 (baseline)**: lr=5e-5, batch size 2, 2 epochs — **best eval loss** (~1.359). This is our reference.\n- **Run2 (lr1e-4)**: higher learning rate, 1 epoch — slightly **worse** eval loss (~1.362); higher LR did not improve validation.\n- **Run3 (batch4)**: batch size 4, 2 epochs — eval loss very close to baseline (~1.359); similar performance.\n\n**Conclusion:** The baseline configuration (run1_baseline) achieved the lowest eval loss. The tuned configs (Run2, Run3) did not improve over baseline; we use **run1_baseline** (saved in `tinyllama_lora_run1/`) as the best model for evaluation and the Gradio UI.","metadata":{}},{"cell_type":"code","source":"!ls -lh","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQSsHL1u7-c4","outputId":"a1f17398-a5bf-4495-d1c2-74769a1ff459","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T03:11:58.258201Z","iopub.execute_input":"2026-02-26T03:11:58.258458Z","iopub.status.idle":"2026-02-26T03:11:58.520725Z","shell.execute_reply.started":"2026-02-26T03:11:58.258431Z","shell.execute_reply":"2026-02-26T03:11:58.519923Z"}},"outputs":[{"name":"stdout","text":"total 5.3M\n-rw-r--r-- 1 root root  370 Feb 26 03:11 experiment_log.csv\n-rw-r--r-- 1 root root 542K Feb 26 02:27 test.jsonl\ndrwxr-xr-x 4 root root 4.0K Feb 26 02:46 tinyllama_lora_run1\ndrwxr-xr-x 3 root root 4.0K Feb 26 02:55 tinyllama_lora_run2\ndrwxr-xr-x 4 root root 4.0K Feb 26 03:11 tinyllama_lora_run3\n-rw-r--r-- 1 root root 4.8M Feb 26 02:27 train.jsonl\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!zip -r tinyllama_lora_run1.zip tinyllama_lora_run1\n!zip -r tinyllama_lora_run2.zip tinyllama_lora_run2\n!zip -r tinyllama_lora_run3.zip tinyllama_lora_run3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WROBqGf68zq8","outputId":"d3917a3b-ed3e-47ca-d3cc-25cc31771d8e","trusted":true,"execution":{"iopub.status.busy":"2026-02-26T03:11:58.522313Z","iopub.execute_input":"2026-02-26T03:11:58.522676Z","iopub.status.idle":"2026-02-26T03:12:08.172895Z","shell.execute_reply.started":"2026-02-26T03:11:58.522629Z","shell.execute_reply":"2026-02-26T03:12:08.172141Z"}},"outputs":[{"name":"stdout","text":"  adding: tinyllama_lora_run1/ (stored 0%)\n  adding: tinyllama_lora_run1/adapter_config.json (deflated 58%)\n  adding: tinyllama_lora_run1/checkpoint-342/ (stored 0%)\n  adding: tinyllama_lora_run1/checkpoint-342/scheduler.pt (deflated 61%)\n  adding: tinyllama_lora_run1/checkpoint-342/adapter_config.json (deflated 58%)\n  adding: tinyllama_lora_run1/checkpoint-342/README.md (deflated 66%)\n  adding: tinyllama_lora_run1/checkpoint-342/optimizer.pt (deflated 8%)\n  adding: tinyllama_lora_run1/checkpoint-342/trainer_state.json (deflated 73%)\n  adding: tinyllama_lora_run1/checkpoint-342/tokenizer_config.json (deflated 46%)\n  adding: tinyllama_lora_run1/checkpoint-342/tokenizer.json (deflated 85%)\n  adding: tinyllama_lora_run1/checkpoint-342/adapter_model.safetensors (deflated 8%)\n  adding: tinyllama_lora_run1/checkpoint-342/scaler.pt (deflated 64%)\n  adding: tinyllama_lora_run1/checkpoint-342/chat_template.jinja (deflated 60%)\n  adding: tinyllama_lora_run1/checkpoint-342/rng_state.pth (deflated 26%)\n  adding: tinyllama_lora_run1/checkpoint-342/training_args.bin (deflated 53%)\n  adding: tinyllama_lora_run1/README.md (deflated 66%)\n  adding: tinyllama_lora_run1/checkpoint-684/ (stored 0%)\n  adding: tinyllama_lora_run1/checkpoint-684/scheduler.pt (deflated 62%)\n  adding: tinyllama_lora_run1/checkpoint-684/adapter_config.json (deflated 58%)\n  adding: tinyllama_lora_run1/checkpoint-684/README.md (deflated 66%)\n  adding: tinyllama_lora_run1/checkpoint-684/optimizer.pt (deflated 8%)\n  adding: tinyllama_lora_run1/checkpoint-684/trainer_state.json (deflated 76%)\n  adding: tinyllama_lora_run1/checkpoint-684/tokenizer_config.json (deflated 46%)\n  adding: tinyllama_lora_run1/checkpoint-684/tokenizer.json (deflated 85%)\n  adding: tinyllama_lora_run1/checkpoint-684/adapter_model.safetensors (deflated 8%)\n  adding: tinyllama_lora_run1/checkpoint-684/scaler.pt (deflated 64%)\n  adding: tinyllama_lora_run1/checkpoint-684/chat_template.jinja (deflated 60%)\n  adding: tinyllama_lora_run1/checkpoint-684/rng_state.pth (deflated 26%)\n  adding: tinyllama_lora_run1/checkpoint-684/training_args.bin (deflated 53%)\n  adding: tinyllama_lora_run1/tokenizer_config.json (deflated 46%)\n  adding: tinyllama_lora_run1/tokenizer.json (deflated 85%)\n  adding: tinyllama_lora_run1/adapter_model.safetensors (deflated 8%)\n  adding: tinyllama_lora_run1/chat_template.jinja (deflated 60%)\n  adding: tinyllama_lora_run1/training_args.bin (deflated 53%)\n  adding: tinyllama_lora_run2/ (stored 0%)\n  adding: tinyllama_lora_run2/adapter_config.json (deflated 58%)\n  adding: tinyllama_lora_run2/checkpoint-342/ (stored 0%)\n  adding: tinyllama_lora_run2/checkpoint-342/scheduler.pt (deflated 62%)\n  adding: tinyllama_lora_run2/checkpoint-342/adapter_config.json (deflated 58%)\n  adding: tinyllama_lora_run2/checkpoint-342/README.md (deflated 66%)\n  adding: tinyllama_lora_run2/checkpoint-342/optimizer.pt (deflated 8%)\n  adding: tinyllama_lora_run2/checkpoint-342/trainer_state.json (deflated 74%)\n  adding: tinyllama_lora_run2/checkpoint-342/tokenizer_config.json (deflated 46%)\n  adding: tinyllama_lora_run2/checkpoint-342/tokenizer.json (deflated 85%)\n  adding: tinyllama_lora_run2/checkpoint-342/adapter_model.safetensors (deflated 8%)\n  adding: tinyllama_lora_run2/checkpoint-342/scaler.pt (deflated 64%)\n  adding: tinyllama_lora_run2/checkpoint-342/chat_template.jinja (deflated 60%)\n  adding: tinyllama_lora_run2/checkpoint-342/rng_state.pth (deflated 26%)\n  adding: tinyllama_lora_run2/checkpoint-342/training_args.bin (deflated 53%)\n  adding: tinyllama_lora_run2/README.md (deflated 66%)\n  adding: tinyllama_lora_run2/tokenizer_config.json (deflated 46%)\n  adding: tinyllama_lora_run2/tokenizer.json (deflated 85%)\n  adding: tinyllama_lora_run2/adapter_model.safetensors (deflated 8%)\n  adding: tinyllama_lora_run2/chat_template.jinja (deflated 60%)\n  adding: tinyllama_lora_run2/training_args.bin (deflated 53%)\n  adding: tinyllama_lora_run3/ (stored 0%)\n  adding: tinyllama_lora_run3/adapter_config.json (deflated 58%)\n  adding: tinyllama_lora_run3/checkpoint-171/ (stored 0%)\n  adding: tinyllama_lora_run3/checkpoint-171/scheduler.pt (deflated 61%)\n  adding: tinyllama_lora_run3/checkpoint-171/adapter_config.json (deflated 58%)\n  adding: tinyllama_lora_run3/checkpoint-171/README.md (deflated 66%)\n  adding: tinyllama_lora_run3/checkpoint-171/optimizer.pt (deflated 8%)\n  adding: tinyllama_lora_run3/checkpoint-171/trainer_state.json (deflated 70%)\n  adding: tinyllama_lora_run3/checkpoint-171/tokenizer_config.json (deflated 46%)\n  adding: tinyllama_lora_run3/checkpoint-171/tokenizer.json (deflated 85%)\n  adding: tinyllama_lora_run3/checkpoint-171/adapter_model.safetensors (deflated 8%)\n  adding: tinyllama_lora_run3/checkpoint-171/scaler.pt (deflated 64%)\n  adding: tinyllama_lora_run3/checkpoint-171/chat_template.jinja (deflated 60%)\n  adding: tinyllama_lora_run3/checkpoint-171/rng_state.pth (deflated 26%)\n  adding: tinyllama_lora_run3/checkpoint-171/training_args.bin (deflated 53%)\n  adding: tinyllama_lora_run3/checkpoint-342/ (stored 0%)\n  adding: tinyllama_lora_run3/checkpoint-342/scheduler.pt (deflated 62%)\n  adding: tinyllama_lora_run3/checkpoint-342/adapter_config.json (deflated 58%)\n  adding: tinyllama_lora_run3/checkpoint-342/README.md (deflated 66%)\n  adding: tinyllama_lora_run3/checkpoint-342/optimizer.pt (deflated 8%)\n  adding: tinyllama_lora_run3/checkpoint-342/trainer_state.json (deflated 73%)\n  adding: tinyllama_lora_run3/checkpoint-342/tokenizer_config.json (deflated 46%)\n  adding: tinyllama_lora_run3/checkpoint-342/tokenizer.json (deflated 85%)\n  adding: tinyllama_lora_run3/checkpoint-342/adapter_model.safetensors (deflated 8%)\n  adding: tinyllama_lora_run3/checkpoint-342/scaler.pt (deflated 64%)\n  adding: tinyllama_lora_run3/checkpoint-342/chat_template.jinja (deflated 60%)\n  adding: tinyllama_lora_run3/checkpoint-342/rng_state.pth (deflated 26%)\n  adding: tinyllama_lora_run3/checkpoint-342/training_args.bin (deflated 53%)\n  adding: tinyllama_lora_run3/README.md (deflated 66%)\n  adding: tinyllama_lora_run3/tokenizer_config.json (deflated 46%)\n  adding: tinyllama_lora_run3/tokenizer.json (deflated 85%)\n  adding: tinyllama_lora_run3/adapter_model.safetensors (deflated 8%)\n  adding: tinyllama_lora_run3/chat_template.jinja (deflated 60%)\n  adding: tinyllama_lora_run3/training_args.bin (deflated 53%)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 9. Evaluation: Perplexity, ROUGE, and Qualitative Comparison\n\nIn this section, we evaluate the fine-tuned **TinyLlama + LoRA** model on the held-out test set and compare it qualitatively against the **base TinyLlama** model.\n\n**Goals (rubric: multiple metrics, thorough analysis):**\n- Compute **perplexity** from the final validation loss: \\(\\text{perplexity} = \\exp(\\text{eval\\_loss})\\).\n- Compute **ROUGE** and **BLEU** on a small subset of `test.jsonl` for generation quality.\n- Show **qualitative examples**: base vs fine-tuned answers on in-domain, upcycling, and out-of-domain questions, with brief analysis.","metadata":{}},{"cell_type":"code","source":"import math\n\n# Compute perplexity from the best eval loss in experiment_df\nif \"experiment_df\" not in globals():\n    raise ValueError(\"experiment_df not found. Run the hyperparameter experiment section first.\")\n\nbest_row = experiment_df.loc[experiment_df[\"run_id\"] == \"run1_baseline\"].iloc[0]\nbest_eval_loss = float(best_row[\"eval_loss\"])\nperplexity = math.exp(best_eval_loss)\n\nprint(\"=== Perplexity from best eval loss ===\")\nprint(f\"Best run ID        : {best_row['run_id']}\")\nprint(f\"Eval loss          : {best_eval_loss:.4f}\")\nprint(f\"Perplexity (exp(L)): {perplexity:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T03:12:08.177166Z","iopub.execute_input":"2026-02-26T03:12:08.177420Z","iopub.status.idle":"2026-02-26T03:12:08.188535Z","shell.execute_reply.started":"2026-02-26T03:12:08.177393Z","shell.execute_reply":"2026-02-26T03:12:08.187695Z"}},"outputs":[{"name":"stdout","text":"=== Perplexity from best eval loss ===\nBest run ID        : run1_baseline\nEval loss          : 1.3590\nPerplexity (exp(L)): 3.89\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Run this cell ONCE before the ROUGE/BLEU cell (required for evaluate.load(\"rouge\") and \"bleu\")\n!pip install -q evaluate rouge_score sacrebleu\nprint(\"Metrics dependencies installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T03:12:08.192145Z","iopub.execute_input":"2026-02-26T03:12:08.192478Z","iopub.status.idle":"2026-02-26T03:12:12.551784Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hMetrics dependencies installed.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ROUGE and BLEU evaluation on a small subset of test.jsonl using the fine-tuned model\n\n# Install metrics dependencies first (if you get rouge_score error, run this cell again, then run the rest)\nimport subprocess\nimport sys\nimport importlib\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"evaluate\", \"rouge_score\", \"sacrebleu\"], check=True)\nimportlib.invalidate_caches()\n\nimport torch\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\nadapter_dir = \"./tinyllama_lora_run1\"  # best fine-tuned run\nbase_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# Load tokenizer (prefer the fine-tuned directory so it matches training)\ntokenizer = AutoTokenizer.from_pretrained(adapter_dir, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load base model and attach LoRA adapter\nbase_model_for_ft = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=\"auto\" if device == \"cuda\" else None,\n)\nft_model = PeftModel.from_pretrained(base_model_for_ft, adapter_dir)\nft_model.eval()\n\n# Load test set from JSONL created earlier (contains instruction/response/text)\ntest_ds = load_dataset(\"json\", data_files=\"test.jsonl\", split=\"train\")\nprint(\"Test set size:\", len(test_ds))\n\nnum_eval = min(50, len(test_ds))\neval_subset = test_ds.select(range(num_eval))\n\npreds, refs = [], []\n\nfor ex in eval_subset:\n    user_q = ex[\"instruction\"]\n    ref = ex[\"response\"]\n\n    prompt = (\n        f\"[SYSTEM] {SYSTEM_PROMPT}\\n\"\n        f\"[USER] {user_q}\\n\"\n        f\"[ASSISTANT]\"\n    )\n\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n    ).to(device)\n\n    with torch.no_grad():\n        gen_ids = ft_model.generate(\n            **inputs,\n            max_new_tokens=256,\n            do_sample=False,\n            temperature=0.7,\n            top_p=0.9,\n        )\n\n    gen_text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n    if \"[ASSISTANT]\" in gen_text:\n        gen_text = gen_text.split(\"[ASSISTANT]\", 1)[-1].strip()\n\n    preds.append(gen_text)\n    refs.append(ref)\n\nrouge = evaluate.load(\"rouge\")\nrouge_scores = rouge.compute(predictions=preds, references=refs)\n\n# BLEU (rubric: multiple metrics including BLEU)\nbleu = evaluate.load(\"bleu\")\nbleu_scores = bleu.compute(predictions=preds, references=[[r] for r in refs])\n\nprint(\"\\n=== ROUGE scores on small test subset ===\")\nfor k, v in rouge_scores.items():\n    print(f\"{k}: {v:.4f}\")\nprint(\"\\n=== BLEU score on same subset ===\")\nprint(f\"bleu: {bleu_scores['bleu']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T03:12:12.553258Z","iopub.execute_input":"2026-02-26T03:12:12.553506Z"}},"outputs":[{"name":"stderr","text":"Model config PreTrainedConfig {\n  \"transformers_version\": \"5.2.0\"\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\nModel config LlamaConfig {\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"dtype\": \"float16\",\n  \"eos_token_id\": 2,\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": null,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_parameters\": {\n    \"rope_theta\": 10000.0,\n    \"rope_type\": \"default\"\n  },\n  \"tie_word_embeddings\": false,\n  \"transformers_version\": \"5.2.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"use_cache\": true\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27503ac4c6844360b2263a3ad9ee96ef"}},"metadata":{}},{"name":"stderr","text":"loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"max_length\": 2048,\n  \"pad_token_id\": 0\n}\n\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p'].\n- `temperature`: `do_sample` is set not to set `True`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n- `top_p`: `do_sample` is set not to set `True`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\nIf you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","output_type":"stream"},{"name":"stdout","text":"Test set size: 305\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Qualitative comparison: base TinyLlama vs fine-tuned TinyLlama + LoRA\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\nadapter_dir = \"./tinyllama_lora_run1\"\nbase_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# Reuse tokenizer from above if available; otherwise load it\nif \"tokenizer\" not in globals():\n    tokenizer = AutoTokenizer.from_pretrained(adapter_dir, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n# Load base TinyLlama (no LoRA)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=\"auto\" if device == \"cuda\" else None,\n)\nbase_model.eval()\n\n# Load a separate TinyLlama copy and attach LoRA adapter for fine-tuned model\nft_base = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=\"auto\" if device == \"cuda\" else None,\n)\nft_model = PeftModel.from_pretrained(ft_base, adapter_dir)\nft_model.eval()\n\n\ndef generate_answer(model, question, max_new_tokens=256):\n    \"\"\"Generate a response from a model given a user question.\"\"\"\n    prompt = (\n        f\"[SYSTEM] {SYSTEM_PROMPT}\\n\"\n        f\"[USER] {question}\\n\"\n        f\"[ASSISTANT]\"\n    )\n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n    ).to(device)\n\n    with torch.no_grad():\n        gen_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            temperature=0.7,\n            top_p=0.9,\n        )\n\n    text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n    if \"[ASSISTANT]\" in text:\n        text = text.split(\"[ASSISTANT]\", 1)[-1].strip()\n    return text\n\n\nsample_questions = [\n    # In-domain scientific question\n    \"How does deforestation affect the carbon cycle and climate change?\",\n    # Upcycling / circular economy question\n    \"What are some sustainable ways to upcycle old plastic bottles at home?\",\n    # Out-of-domain question (should trigger refusal)\n    \"Who won the FIFA World Cup in 2022?\",\n]\n\nfor q in sample_questions:\n    print(\"=\" * 100)\n    print(\"Question:\", q)\n\n    base_ans = generate_answer(base_model, q)\n    ft_ans = generate_answer(ft_model, q)\n\n    print(\"\\n[Base TinyLlama answer]\\n\")\n    print(base_ans[:800])\n\n    print(\"\\n[Fine-tuned TinyLlama + LoRA answer]\\n\")\n    print(ft_ans[:800])\n\n    print(\"\\n(Note: Evaluate which answer is more domain-specific, accurate, \"\n          \"and whether the fine-tuned model handles out-of-domain queries better.)\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interpretation of Metrics and Hyperparameter Tuning\n\n- **Perplexity**: We convert the best validation loss from the Trainer (around 1.36 for `run1_baseline`) into perplexity via \\(\\exp(\\text{loss})\\), which gives a perplexity slightly above 3. This indicates the fine-tuned model assigns relatively high probability to the correct target tokens on held-out environmental questions.\n- **ROUGE scores**: On a small subset of `test.jsonl`, we compute ROUGE (e.g., ROUGE-1/ROUGE-L). These scores measure n-gram overlap between generated answers and reference answers; moderate but consistent ROUGE values show the model is capturing the main content of environmental explanations but does not perfectly match wording.\n- **BLEU score**: We also report BLEU on the same subset (rubric: multiple metrics). BLEU measures precision of n-gram matches between generation and reference; together with ROUGE and perplexity, this gives a fuller picture of generation quality.\n- **Hyperparameter experiments**: The experiment table compares three configurations (learning rate 5e-5 vs 1e-4, batch size 2 vs 4, epochs 1–2). The baseline configuration (lr=5e-5, batch size 2, 2 epochs) achieved the **lowest eval loss** with reasonable training time and GPU memory. Higher learning rate or larger per-device batch size did **not** improve validation metrics, which we document explicitly in the table. We use the best run (run1_baseline) for evaluation and deployment (rubric: thorough hyperparameter exploration, experiment table).\n- **Rubric alignment**: We report **multiple evaluation metrics** (eval loss, perplexity, ROUGE, BLEU) and **qualitative testing** (base vs fine-tuned comparison on in-domain, upcycling, and out-of-domain questions). This thorough analysis shows how the fine-tuned chatbot performs on environmental QA and refusal behavior relative to the base model.","metadata":{}},{"cell_type":"markdown","source":"## 10. Deployment – Gradio Chat Interface\n\nThis section provides an interactive **web UI** (using **Gradio**) so users can chat with the fine-tuned **Environmental Science & Circular Economy Assistant** directly from the notebook. The interface:\n\n- Uses the **TinyLlama-1.1B-Chat + LoRA** weights from `tinyllama_lora_run1/`.\n- Applies the same `[SYSTEM] / [USER] / [ASSISTANT]` chat format used during training.\n- Clearly communicates that the assistant is **domain-specific** (environmental science, sustainability, circular economy) and will politely refuse out-of-domain questions.","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\nsource_dir = \"/kaggle/input/datasets/favourakinwande/model-run-1/tinyllama_lora_run1\"\ntarget_dir = \"/kaggle/working/tinyllama_lora_run1\"\n\nif not os.path.exists(target_dir):\n    shutil.copytree(source_dir, target_dir)\n\nprint(\"Copied successfully \")\nprint(\"Working dir contains:\", os.listdir(\"/kaggle/working\"))\nprint(\"Adapter dir contains:\", os.listdir(target_dir)[:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T06:49:16.939198Z","iopub.execute_input":"2026-02-26T06:49:16.939934Z","iopub.status.idle":"2026-02-26T06:49:16.945455Z","shell.execute_reply.started":"2026-02-26T06:49:16.939904Z","shell.execute_reply":"2026-02-26T06:49:16.944657Z"}},"outputs":[{"name":"stdout","text":"Copied successfully \nWorking dir contains: ['tinyllama_lora_run1', '.virtual_documents']\nAdapter dir contains: ['adapter_config.json', 'README.md', 'checkpoint-684', 'tokenizer.json', 'adapter_model.safetensors', 'chat_template.jinja', 'tokenizer_config.json', 'checkpoint-342', 'training_args.bin']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip -q install gradio transformers peft accelerate\n\nimport gradio as gr\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport os\n\n# ---- REQUIRED: define SYSTEM_PROMPT (you were missing this) ----\nSYSTEM_PROMPT = (\n    \"You are an Environmental Science & Circular Economy assistant. \"\n    \"Answer questions about environmental science, climate change, sustainability, recycling, and upcycling. \"\n    \"If a question is off-topic, politely say you can only help with these topics.\"\n)\n\nui_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device for UI:\", ui_device)\n\nui_adapter_dir = \"/kaggle/working/tinyllama_lora_run1\"\nui_base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# Load tokenizer: prefer adapter dir IF it contains tokenizer files, otherwise use base model tokenizer\ndef has_tokenizer_files(path: str) -> bool:\n    needed = [\"tokenizer.json\", \"tokenizer.model\", \"vocab.json\", \"merges.txt\"]\n    return any(os.path.exists(os.path.join(path, f)) for f in needed)\n\ntokenizer_source = ui_adapter_dir if has_tokenizer_files(ui_adapter_dir) else ui_base_model_name\nprint(\"Tokenizer loaded from:\", tokenizer_source)\n\n# Load tokenizer and model once\nif \"ui_tokenizer\" not in globals() or \"ui_model\" not in globals():\n    ui_tokenizer = AutoTokenizer.from_pretrained(tokenizer_source, use_fast=True)\n    if ui_tokenizer.pad_token is None:\n        ui_tokenizer.pad_token = ui_tokenizer.eos_token\n\n    base_for_ui = AutoModelForCausalLM.from_pretrained(\n        ui_base_model_name,\n        torch_dtype=torch.float16 if ui_device == \"cuda\" else torch.float32,\n        device_map=\"auto\" if ui_device == \"cuda\" else None,\n    )\n\n    ui_model = PeftModel.from_pretrained(base_for_ui, ui_adapter_dir)\n    ui_model.eval()\n\n\ndef build_prompt(message, history):\n    \"\"\"Build a chat-style prompt including history using the training format.\"\"\"\n    lines = [f\"[SYSTEM] {SYSTEM_PROMPT}\"]\n    for old_user, old_assistant in history:\n        lines.append(f\"[USER] {old_user}\")\n        lines.append(f\"[ASSISTANT] {old_assistant}\")\n    lines.append(f\"[USER] {message}\")\n    lines.append(\"[ASSISTANT]\")\n    return \"\\n\".join(lines)\n\n\ndef chat_fn(message, history):\n    prompt = build_prompt(message, history)\n\n    inputs = ui_tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=512,\n        padding=True,\n    ).to(ui_device)\n\n    with torch.no_grad():\n        gen_ids = ui_model.generate(\n            **inputs,\n            max_new_tokens=256,\n            do_sample=True,     # <- now temperature/top_p actually apply\n            temperature=0.7,\n            top_p=0.9,\n            eos_token_id=ui_tokenizer.eos_token_id,\n            pad_token_id=ui_tokenizer.pad_token_id,\n        )\n\n    text = ui_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n\n    # Keep only the last assistant segment\n    if \"[ASSISTANT]\" in text:\n        text = text.split(\"[ASSISTANT]\")[-1].strip()\n    return text\n\n\ninstructions_md = \"\"\"\n**How to use**\n- Type your question and press Enter (or Submit).\n- **Supported topics:** environmental science, climate change, sustainability, circular economy, upcycling, recycling.\n- The assistant will answer clearly and may politely decline off-topic questions.\n\"\"\"\n\nwith gr.Blocks(title=\"Environmental Science & Circular Economy Assistant\") as demo:\n    gr.Markdown(\"# Environmental Science & Circular Economy Assistant (TinyLlama + LoRA)\")\n    gr.Markdown(instructions_md)\n    gr.ChatInterface(fn=chat_fn)\n\ndemo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T06:49:34.957710Z","iopub.execute_input":"2026-02-26T06:49:34.958448Z","iopub.status.idle":"2026-02-26T06:49:50.907645Z","shell.execute_reply.started":"2026-02-26T06:49:34.958408Z","shell.execute_reply":"2026-02-26T06:49:50.906857Z"}},"outputs":[{"name":"stdout","text":"Using device for UI: cuda\nTokenizer loaded from: /kaggle/working/tinyllama_lora_run1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1a95ecba914ef3aa38ee7d90e281fa"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23bb90b33ca48d7b69b25daa4ee8858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98f23fcb4f804ad189fbbc0dafc3a21a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be373cdd4688493e9b8cbf4492e4d1ac"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  self.chatbot = Chatbot(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://814ce918e07668e9c4.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://814ce918e07668e9c4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":13}]}